{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "team08.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FaWaYXG1fIRm",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms\n",
        "import torch.utils.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GA4Aiz2Giu5e",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(0)\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MjS3A1UNFJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GgHZsI0VyaUJ"
      },
      "source": [
        "# TODO\n",
        "\n",
        "* Use `model.train()` `model.eval()` appropriately\n",
        "* Training and validation loss curves\n",
        "* Early stopping at minimum validation loss\n",
        "* Plot with loss curves for autoencoder with different bottleneck sizes\n",
        "* Maybe regularization methods at transfer-learning step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iyK3i7pS9gSH"
      },
      "source": [
        "# Exercise 1: Convolutional Autoencoder\n",
        "\n",
        "Steps:\n",
        "1. Load MNIST train and test sets. Split the original training data into 95% training and 5% validation data.\n",
        "2. Implement a convolutional autoencoder (with separate Encoder and Decoder modules).\n",
        "3. Train the convolutional autoencoder, with different bottleneck sizes. Plot the train and validation loss curves of all autoencoders in the same figure.\n",
        "4. Compute the avg. image reconstruction error (MSE) of the trained models on the MNIST validation and test sets. Show the results in a table, including #params of each model.\n",
        "5. Select one of the autoencoders and feed it 5 random MNIST images from the test set. Show them along with their reconstructions.\n",
        "Generate 5 new images by injecting random values as input to the decoder. Show them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Vf3xEV29k0y"
      },
      "source": [
        "## Module definition\n",
        "\n",
        "Our autoencoder will be defined by the `ConvolutionalAutoencoder` class which uses a `ConvolutionalEncoder` object to encode followed a `ConvolutionalDecoder` object to decode. The `ConvolutionalEncoder` and `ConvolutionalDecoder` classes make use of `n_blocks` `ConvolutionalBlock`s or `DeconvolutionalBlock`s which are composed of `layer_per_block` convolution layers with the same number of filters.\n",
        "\n",
        "The dimensionality is reduced by applying 2-factor spatial downsampling at each block. The number of filters is doubled for each subsequent block. The decoder makes the exact oposite process.\n",
        "\n",
        "The final layer uses a tanh activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EknaWIjxJUZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load models.py\n",
        "import torch\n",
        "import torch.nn\n",
        "\n",
        "\n",
        "class ConvolutionalAutoencoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_blocks, downsampling_method, upsampling_method, layers_per_block=2):\n",
        "        super().__init__()\n",
        "        self.n_blocks = n_blocks\n",
        "        self.downsampling_method = downsampling_method\n",
        "        self.upsampling_method = upsampling_method\n",
        "\n",
        "        self.encoder = ConvolutionalEncoder(n_blocks, downsampling_method, layers_per_block=layers_per_block)\n",
        "        self.decoder = ConvolutionalDecoder(n_blocks, upsampling_method, self.encoder.output_channels,\n",
        "                                            layers_per_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        code = self.encoder(x)\n",
        "        reconstruction = self.decoder(code)\n",
        "        return reconstruction\n",
        "\n",
        "\n",
        "class ConvolutionalAutoencoderReducedLatentDim(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_shape, n_blocks, downsampling_method, upsampling_method, layers_per_block=2,\n",
        "                 latent_dimensionality=50):\n",
        "        super().__init__()\n",
        "        self.n_blocks = n_blocks\n",
        "        self.downsampling_method = downsampling_method\n",
        "        self.upsampling_method = upsampling_method\n",
        "        self.latent_dimensionality = latent_dimensionality\n",
        "\n",
        "        # Encoder: Convolutional blocks + Linear\n",
        "        self.convolutional_encoder = ConvolutionalEncoder(\n",
        "            n_blocks, downsampling_method, layers_per_block=layers_per_block)\n",
        "        self.output_shape = (self.convolutional_encoder.init_filters * 2 ** (n_blocks - 1),\n",
        "                             input_shape[0] // 2 ** n_blocks,\n",
        "                             input_shape[1] // 2 ** n_blocks)\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            self.convolutional_encoder,\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(self.output_shape[0] * self.output_shape[1] * self.output_shape[2],\n",
        "                            latent_dimensionality),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        # Decoder: Linear + Convolutional blocks\n",
        "        self.linear_decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_dimensionality, self.output_shape[0] * self.output_shape[1] * self.output_shape[2]),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.convolutional_decoder = ConvolutionalDecoder(n_blocks, upsampling_method,\n",
        "                                                          self.convolutional_encoder.output_channels,\n",
        "                                                          layers_per_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        code = self.encoder(x)\n",
        "        reconstruction = self.convolutional_decoder(self.linear_decoder(code).view((-1,) + self.output_shape))\n",
        "        return reconstruction\n",
        "\n",
        "\n",
        "class ConvolutionalEncoder(torch.nn.Module):\n",
        "    DOWNSAMPLING_METHODS = [\"max-pooling\", \"avg-pooling\", \"stride-2\"]\n",
        "\n",
        "    def __init__(self, n_blocks, downsampling_method, init_filters=16, layers_per_block=2, kernel_size=5,\n",
        "                 input_channels=1):\n",
        "        super().__init__()\n",
        "        self.n_blocks = n_blocks\n",
        "        assert downsampling_method in self.DOWNSAMPLING_METHODS\n",
        "        self.downsampling_method = downsampling_method\n",
        "        self.layers_per_block = layers_per_block\n",
        "        self.init_filters = init_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.input_channels = input_channels\n",
        "\n",
        "        # First layer so we have <input_channels> channels.\n",
        "        n_filters = init_filters\n",
        "        layers = [ConvolutionalBlock(input_channels, n_filters, kernel_size, 1)]\n",
        "\n",
        "        # Encoding blocks.\n",
        "        input_channels = n_filters\n",
        "        for _ in range(n_blocks):\n",
        "            if downsampling_method == \"max-pooling\":\n",
        "                # Convolutional block + max pooling.\n",
        "                conv_block = torch.nn.Sequential(\n",
        "                    ConvolutionalBlock(input_channels, n_filters, kernel_size, layers_per_block),\n",
        "                    torch.nn.MaxPool2d(2)\n",
        "                )\n",
        "            elif downsampling_method == \"avg-pooling\":\n",
        "                # Convolutional block + average pooling.\n",
        "                conv_block = torch.nn.Sequential(\n",
        "                    ConvolutionalBlock(input_channels, n_filters, kernel_size, layers_per_block),\n",
        "                    torch.nn.AvgPool2d(2)\n",
        "                )\n",
        "            else:\n",
        "                # Stride-2 convolution.\n",
        "                conv_block = ConvolutionalBlock(input_channels, n_filters, kernel_size,\n",
        "                                                layers_per_block, last_stride=2)\n",
        "            layers.append(conv_block)\n",
        "            # Double the number of filters.\n",
        "            input_channels = n_filters\n",
        "            n_filters = 2 * n_filters\n",
        "\n",
        "        self.encoder = torch.nn.Sequential(*layers)\n",
        "        self.output_channels = input_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "class ConvolutionalDecoder(torch.nn.Module):\n",
        "    UPSAMPLING_METHODS = [\"transposed\", \"bilinear\", \"bicubic\", \"nearest\"]\n",
        "\n",
        "    def __init__(self, n_blocks, upsampling_method, input_channels, layers_per_block=2, kernel_size=5,\n",
        "                 output_channels=1):\n",
        "        super().__init__()\n",
        "        self.n_blocks = n_blocks\n",
        "        assert upsampling_method in self.UPSAMPLING_METHODS\n",
        "        self.upsampling_method = upsampling_method\n",
        "        self.layers_per_block = layers_per_block\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # Decoding blocks.\n",
        "        n_filters = input_channels\n",
        "        for _ in range(n_blocks):\n",
        "            if upsampling_method == \"transposed\":\n",
        "                # Deconvolutional block\n",
        "                conv_block = DeconvolutionalBlock(input_channels, n_filters, kernel_size, layers_per_block,\n",
        "                                                  stride=2)\n",
        "            else:\n",
        "                # Upsampling.\n",
        "                conv_block = torch.nn.Sequential(\n",
        "                    ConvolutionalBlock(input_channels, n_filters, kernel_size, layers_per_block),\n",
        "                    torch.nn.Upsample(scale_factor=2, mode=upsampling_method)\n",
        "                )\n",
        "            layers.append(conv_block)\n",
        "            # Half the number of filters.\n",
        "            input_channels = n_filters\n",
        "            n_filters = n_filters // 2\n",
        "\n",
        "        # Last layer so we have <output_channels> channel.\n",
        "        layers.append(torch.nn.Conv2d(input_channels, output_channels, kernel_size, padding=kernel_size // 2))\n",
        "        layers.append(torch.nn.Tanh())\n",
        "\n",
        "        self.decoder = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "\n",
        "class ConvolutionalBlock(torch.nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    Applies n_layers convolutional layers with the same number of\n",
        "    filters and filter sizes with ReLU activations\n",
        "    keeping the same spacial size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channels, n_filters, kernel_size, n_layers,\n",
        "                 last_stride=1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        padding = kernel_size // 2  # To keep the same size.\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            if i == 0:  # First layer with correct input channels.\n",
        "                layers.append(torch.nn.Conv2d(input_channels, n_filters, kernel_size, padding=padding))\n",
        "            elif 0 < i < n_layers:  # Intermediate layers.\n",
        "                layers.append(torch.nn.Conv2d(n_filters, n_filters, kernel_size, padding=padding))\n",
        "            else:  # Last layer with stride.\n",
        "                layers.append(torch.nn.Conv2d(n_filters, n_filters, kernel_size, last_stride, padding))\n",
        "            layers.append(torch.nn.ReLU())\n",
        "\n",
        "        # To sequentially apply the layers.\n",
        "        self.block = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class DeconvolutionalBlock(torch.nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    Applies a transposed convolution followed by n_layers-1 convolutional\n",
        "    layers with the same number of filters and filter sizes with ReLU\n",
        "    activations keeping the same spacial size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channels, n_filters, kernel_size, n_layers, stride):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        # Transposed convolution layer.\n",
        "        layers.append(torch.nn.ConvTranspose2d(input_channels, n_filters, kernel_size, stride, padding, 1))\n",
        "        layers.append(torch.nn.ReLU())\n",
        "\n",
        "        for _ in range(n_layers - 1):\n",
        "            layers.append(torch.nn.Conv2d(n_filters, n_filters, kernel_size, padding=padding))\n",
        "            layers.append(torch.nn.ReLU())\n",
        "\n",
        "        # To sequentially apply the layers.\n",
        "        self.block = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u_ZD0rgWBhJa"
      },
      "source": [
        "To do a quick test, we will pass a random image and check if the output is of the same size as the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hIO0AZWCfIR2",
        "colab": {}
      },
      "source": [
        "image = torch.randn((10, 1, 128, 128))\n",
        "autoencoder = ConvolutionalAutoencoderReducedLatentDim(input_shape=(128, 128),\n",
        "                                                       n_blocks=2,\n",
        "                                                       downsampling_method='max-pooling',\n",
        "                                                       upsampling_method='nearest',\n",
        "                                                       layers_per_block=2,\n",
        "                                                       latent_dimensionality=50\n",
        ")\n",
        "output = autoencoder(image)\n",
        "assert output.shape == (10, 1, 128, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z25DsijeCNvi"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We split the training set and normalize the input images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ObQndlPufIR9",
        "colab": {}
      },
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.1307,), (0.3081,))  # Mean and std from internet...\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1VIGvzG4fISD",
        "colab": {}
      },
      "source": [
        "mnist = torchvision.datasets.MNIST('mnist_dataset', train=True, transform=transform, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K5cYwBL2fISH",
        "colab": {}
      },
      "source": [
        "dataset_len = len(mnist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPrTXSnZfISN",
        "outputId": "c784954b-e96a-4215-d33a-95c5d1902ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset_len * 0.95"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57000.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B1BwFJY_fISU",
        "colab": {}
      },
      "source": [
        "mnist_train, mnist_val = torch.utils.data.random_split(mnist, [57000, dataset_len - 57000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b5yH0O1kfISY",
        "colab": {}
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(mnist_train,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ybj6UC1qJUaN",
        "colab": {}
      },
      "source": [
        "val_dataloader = torch.utils.data.DataLoader(mnist_val,\n",
        "                                             batch_size=1,\n",
        "                                             num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqt6_aCtJUaR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31f5a7d2-7525-4c8a-adef-b43922e167c3"
      },
      "source": [
        "mnist_train[0][0].shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sIYstBcDJUaV",
        "colab": {}
      },
      "source": [
        "autoencoder = ConvolutionalAutoencoderReducedLatentDim(\n",
        "    input_shape=(28, 28),\n",
        "    n_blocks=2,\n",
        "    downsampling_method='max-pooling',\n",
        "    upsampling_method='nearest',\n",
        "    layers_per_block=2,\n",
        "    latent_dimensionality=50\n",
        ").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fNNQ25Z-fISe",
        "colab": {}
      },
      "source": [
        "# We use 'sum' as reduction to compute the image reconstruction error\n",
        "# instead of the pixel reconstruction error.\n",
        "mse = torch.nn.MSELoss(reduction='sum')\n",
        "adam = torch.optim.Adam(autoencoder.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AEwsot_KfISi",
        "colab": {}
      },
      "source": [
        "def train(model, dataloader, criterion, optimizer, epoch, loss_history):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        images = batch[0].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed = model(images)\n",
        "        loss = criterion(reconstructed, images) / BATCH_SIZE\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_history.append(loss.item())\n",
        "        if i % 100 == 0:\n",
        "            print('[%d, %d] loss: %.3f' % (epoch + 1, i, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GoV4rvPyJUac",
        "colab": {}
      },
      "source": [
        "def test(model, dataloader, criterion, epoch, loss_history):\n",
        "    loss = 0\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        images = batch[0].to(device)\n",
        "        reconstructed = model(images)\n",
        "        loss += criterion(reconstructed, images).item()\n",
        "\n",
        "    mean_loss = loss / (i + 1)\n",
        "    loss_history.append(mean_loss)  \n",
        "    print('[%d, validation] loss: %.3f' % (epoch + 1, mean_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CXQlTiq2fISm",
        "outputId": "114b7057-5fd2-474f-b6ba-1ef20e123da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    autoencoder.eval()\n",
        "    with torch.no_grad():\n",
        "        test(autoencoder, val_dataloader, mse, epoch, validation_loss)\n",
        "    autoencoder.train()\n",
        "    train(autoencoder, train_dataloader, mse, adam, epoch, training_loss)\n",
        "\n",
        "autoencoder.eval()\n",
        "with torch.no_grad():\n",
        "    test(autoencoder, val_dataloader, mse, epoch, validation_loss)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, validation] loss: 796.878\n",
            "[1, 0] loss: 797.393\n",
            "[1, 100] loss: 517.857\n",
            "[1, 200] loss: 447.814\n",
            "[1, 300] loss: 388.369\n",
            "[1, 400] loss: 334.320\n",
            "[1, 500] loss: 336.224\n",
            "[1, 600] loss: 340.294\n",
            "[1, 700] loss: 287.889\n",
            "[1, 800] loss: 319.420\n",
            "[1, 900] loss: 266.112\n",
            "[1, 1000] loss: 309.234\n",
            "[1, 1100] loss: 285.864\n",
            "[1, 1200] loss: 291.301\n",
            "[1, 1300] loss: 294.726\n",
            "[1, 1400] loss: 317.546\n",
            "[1, 1500] loss: 307.681\n",
            "[1, 1600] loss: 291.154\n",
            "[1, 1700] loss: 300.295\n",
            "[2, validation] loss: 298.423\n",
            "[2, 0] loss: 296.658\n",
            "[2, 100] loss: 262.399\n",
            "[2, 200] loss: 245.063\n",
            "[2, 300] loss: 282.054\n",
            "[2, 400] loss: 291.981\n",
            "[2, 500] loss: 265.405\n",
            "[2, 600] loss: 314.479\n",
            "[2, 700] loss: 259.582\n",
            "[2, 800] loss: 278.393\n",
            "[2, 900] loss: 228.147\n",
            "[2, 1000] loss: 261.743\n",
            "[2, 1100] loss: 280.236\n",
            "[2, 1200] loss: 305.498\n",
            "[2, 1300] loss: 335.973\n",
            "[2, 1400] loss: 268.991\n",
            "[2, 1500] loss: 286.621\n",
            "[2, 1600] loss: 279.265\n",
            "[2, 1700] loss: 267.546\n",
            "[3, validation] loss: 288.585\n",
            "[3, 0] loss: 287.620\n",
            "[3, 100] loss: 294.265\n",
            "[3, 200] loss: 322.566\n",
            "[3, 300] loss: 272.285\n",
            "[3, 400] loss: 308.980\n",
            "[3, 500] loss: 269.108\n",
            "[3, 600] loss: 281.992\n",
            "[3, 700] loss: 278.564\n",
            "[3, 800] loss: 274.175\n",
            "[3, 900] loss: 268.205\n",
            "[3, 1000] loss: 313.970\n",
            "[3, 1100] loss: 291.748\n",
            "[3, 1200] loss: 301.454\n",
            "[3, 1300] loss: 283.257\n",
            "[3, 1400] loss: 299.328\n",
            "[3, 1500] loss: 287.819\n",
            "[3, 1600] loss: 298.169\n",
            "[3, 1700] loss: 301.850\n",
            "[4, validation] loss: 285.417\n",
            "[4, 0] loss: 261.220\n",
            "[4, 100] loss: 269.842\n",
            "[4, 200] loss: 305.492\n",
            "[4, 300] loss: 275.648\n",
            "[4, 400] loss: 276.276\n",
            "[4, 500] loss: 261.514\n",
            "[4, 600] loss: 269.913\n",
            "[4, 700] loss: 292.945\n",
            "[4, 800] loss: 281.059\n",
            "[4, 900] loss: 307.733\n",
            "[4, 1000] loss: 284.549\n",
            "[4, 1100] loss: 261.088\n",
            "[4, 1200] loss: 314.979\n",
            "[4, 1300] loss: 284.402\n",
            "[4, 1400] loss: 267.810\n",
            "[4, 1500] loss: 281.003\n",
            "[4, 1600] loss: 309.215\n",
            "[4, 1700] loss: 284.438\n",
            "[5, validation] loss: 283.844\n",
            "[5, 0] loss: 283.658\n",
            "[5, 100] loss: 294.832\n",
            "[5, 200] loss: 283.271\n",
            "[5, 300] loss: 251.442\n",
            "[5, 400] loss: 254.118\n",
            "[5, 500] loss: 248.005\n",
            "[5, 600] loss: 289.137\n",
            "[5, 700] loss: 291.904\n",
            "[5, 800] loss: 289.411\n",
            "[5, 900] loss: 289.464\n",
            "[5, 1000] loss: 274.730\n",
            "[5, 1100] loss: 282.148\n",
            "[5, 1200] loss: 300.312\n",
            "[5, 1300] loss: 265.047\n",
            "[5, 1400] loss: 281.274\n",
            "[5, 1500] loss: 283.642\n",
            "[5, 1600] loss: 257.455\n",
            "[5, 1700] loss: 236.073\n",
            "[6, validation] loss: 281.963\n",
            "[6, 0] loss: 280.671\n",
            "[6, 100] loss: 248.363\n",
            "[6, 200] loss: 248.047\n",
            "[6, 300] loss: 266.751\n",
            "[6, 400] loss: 263.589\n",
            "[6, 500] loss: 267.603\n",
            "[6, 600] loss: 286.042\n",
            "[6, 700] loss: 279.585\n",
            "[6, 800] loss: 271.346\n",
            "[6, 900] loss: 284.202\n",
            "[6, 1000] loss: 257.419\n",
            "[6, 1100] loss: 281.607\n",
            "[6, 1200] loss: 279.326\n",
            "[6, 1300] loss: 257.927\n",
            "[6, 1400] loss: 292.923\n",
            "[6, 1500] loss: 303.340\n",
            "[6, 1600] loss: 313.970\n",
            "[6, 1700] loss: 283.172\n",
            "[7, validation] loss: 283.530\n",
            "[7, 0] loss: 279.828\n",
            "[7, 100] loss: 241.532\n",
            "[7, 200] loss: 287.082\n",
            "[7, 300] loss: 296.509\n",
            "[7, 400] loss: 260.274\n",
            "[7, 500] loss: 261.458\n",
            "[7, 600] loss: 288.139\n",
            "[7, 700] loss: 257.258\n",
            "[7, 800] loss: 265.066\n",
            "[7, 900] loss: 286.217\n",
            "[7, 1000] loss: 290.070\n",
            "[7, 1100] loss: 310.223\n",
            "[7, 1200] loss: 269.620\n",
            "[7, 1300] loss: 272.714\n",
            "[7, 1400] loss: 280.840\n",
            "[7, 1500] loss: 267.640\n",
            "[7, 1600] loss: 316.333\n",
            "[7, 1700] loss: 299.198\n",
            "[8, validation] loss: 281.523\n",
            "[8, 0] loss: 291.396\n",
            "[8, 100] loss: 289.696\n",
            "[8, 200] loss: 248.972\n",
            "[8, 300] loss: 262.216\n",
            "[8, 400] loss: 265.266\n",
            "[8, 500] loss: 261.008\n",
            "[8, 600] loss: 258.979\n",
            "[8, 700] loss: 278.248\n",
            "[8, 800] loss: 264.405\n",
            "[8, 900] loss: 251.703\n",
            "[8, 1000] loss: 245.776\n",
            "[8, 1100] loss: 326.921\n",
            "[8, 1200] loss: 271.083\n",
            "[8, 1300] loss: 272.339\n",
            "[8, 1400] loss: 297.319\n",
            "[8, 1500] loss: 287.221\n",
            "[8, 1600] loss: 291.971\n",
            "[8, 1700] loss: 272.432\n",
            "[9, validation] loss: 279.345\n",
            "[9, 0] loss: 245.922\n",
            "[9, 100] loss: 278.194\n",
            "[9, 200] loss: 269.062\n",
            "[9, 300] loss: 258.828\n",
            "[9, 400] loss: 269.179\n",
            "[9, 500] loss: 295.712\n",
            "[9, 600] loss: 274.261\n",
            "[9, 700] loss: 261.661\n",
            "[9, 800] loss: 271.869\n",
            "[9, 900] loss: 257.629\n",
            "[9, 1000] loss: 299.094\n",
            "[9, 1100] loss: 291.404\n",
            "[9, 1200] loss: 285.446\n",
            "[9, 1300] loss: 267.215\n",
            "[9, 1400] loss: 304.633\n",
            "[9, 1500] loss: 286.302\n",
            "[9, 1600] loss: 270.270\n",
            "[9, 1700] loss: 251.875\n",
            "[10, validation] loss: 279.620\n",
            "[10, 0] loss: 282.186\n",
            "[10, 100] loss: 248.360\n",
            "[10, 200] loss: 285.108\n",
            "[10, 300] loss: 283.065\n",
            "[10, 400] loss: 288.752\n",
            "[10, 500] loss: 276.185\n",
            "[10, 600] loss: 267.274\n",
            "[10, 700] loss: 262.851\n",
            "[10, 800] loss: 283.072\n",
            "[10, 900] loss: 293.618\n",
            "[10, 1000] loss: 262.280\n",
            "[10, 1100] loss: 270.927\n",
            "[10, 1200] loss: 273.240\n",
            "[10, 1300] loss: 276.121\n",
            "[10, 1400] loss: 288.794\n",
            "[10, 1500] loss: 303.815\n",
            "[10, 1600] loss: 253.202\n",
            "[10, 1700] loss: 260.957\n",
            "[10, validation] loss: 278.856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OKrrQKRfISs",
        "colab": {}
      },
      "source": [
        "mnist_test = torchvision.datasets.MNIST('mnist_dataset', train=False, transform=transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmq0GFneiCDo",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I6vt-OjST-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e6783098-8e49-46ba-94a8-6abc8a056529"
      },
      "source": [
        "plt.plot(training_loss, label='training loss')\n",
        "plt.plot(np.arange(len(validation_loss)) * len(mnist_train) / BATCH_SIZE, validation_loss, label='validation loss')\n",
        "plt.legend()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2ec3ec79b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DQh4_KZ8h5Aj",
        "colab": {}
      },
      "source": [
        "reconstructed = autoencoder(mnist_test[6][0].to(device).unsqueeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dZnZDQXqiGgv",
        "outputId": "12aaaa78-8147-46b8-807c-d20cfe88b61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.imshow(mnist_test[6][0][0], cmap='gray')\n",
        "plt.subplot(122)\n",
        "plt.imshow(reconstructed[0][0].detach().cpu().numpy(), cmap='gray')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2ec3fbdfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAC4CAYAAAD61bdSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAUcAApFAQeAQlHAAaBQFHAAKNT/AdKslyWNziMjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ODN1xHIHEL",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 2: Transfer Learning\n",
        "\n",
        "## Steps:\n",
        "\n",
        "1. Select a subset of 100 images and their associated labels from the MNIST training data.\n",
        "2. Select one of the previously trained autoencoders.\n",
        "3. Create a digit (0-9) classification model reusing the encoder of the autoencoder and adding the needed fully connected (projection) layer.\n",
        "3. Pretraining: use the weights of the autoencoder as initial values for the network weights and train a classification model on the subset of 100 samples.\n",
        "4. Fine-tuning: do the same, but train the new projection layer with a normal learning rate and the reused part with a very low learning rate.\n",
        "5. From scratch: train the model on the 100 samples without reusing the decoder weights at all.\n",
        "6. Show the accuracy of the four models on the MNIST test set in a table.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oYtzXP8IUh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = torch.nn.Linear(50, 10).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adq2d7EnJaW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = torch.nn.CrossEntropyLoss()\n",
        "adam = torch.optim.Adam(classifier.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8UWsjOhJqKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_subset = torch.utils.data.Subset(mnist_train, range(100))\n",
        "subset_dataloader = torch.utils.data.DataLoader(training_subset,\n",
        "                                                batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UWTRqkZ6MEgf",
        "colab": {}
      },
      "source": [
        "def classification_train(classifier, encoder, dataloader, criterion, optimizer, epoch, loss_history):\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        images, labels = images.to(device), labels.to(device, torch.long)\n",
        "        optimizer.zero_grad()\n",
        "        # To latent space.\n",
        "        codes = encoder(images).detach()\n",
        "        # Classify from latent space.\n",
        "        outputs = classifier(codes)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_history.append(loss.item())\n",
        "        print('[%d, %d] loss: %.3f' % (epoch + 1, i, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv5E5Xc6QiKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classification_test(classifier, encoder, dataloader, criterion, epoch, loss_history):\n",
        "    loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        images, labels = images.to(device), labels.to(device, torch.long)\n",
        "        codes = encoder(images).detach()\n",
        "        outputs = classifier(codes)\n",
        "        loss += criterion(outputs, labels).item()\n",
        "        # Compute average accuracy.\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    mean_loss = loss / (i + 1)\n",
        "    loss_history.append(mean_loss)  \n",
        "    print('[%d, validation] loss: %.3f accuracy: %.3f' % (epoch + 1, mean_loss, 100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx50jNQdNaoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b422c8b1-51ae-4623-b9d5-09a0f208366d"
      },
      "source": [
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "for epoch in range(80):\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        classification_test(classifier, autoencoder.encoder, val_dataloader, cross_entropy, epoch, validation_loss)\n",
        "    classifier.train()\n",
        "    classification_train(classifier, autoencoder.encoder, subset_dataloader, cross_entropy, adam, epoch, training_loss)\n",
        "\n",
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    classification_test(classifier, autoencoder.encoder, val_dataloader, cross_entropy, epoch, validation_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, validation] loss: 2.461 accuracy: 11.800\n",
            "[1, 0] loss: 2.427\n",
            "[1, 1] loss: 2.292\n",
            "[1, 2] loss: 2.422\n",
            "[1, 3] loss: 2.251\n",
            "[2, validation] loss: 2.252 accuracy: 18.200\n",
            "[2, 0] loss: 2.208\n",
            "[2, 1] loss: 2.172\n",
            "[2, 2] loss: 2.154\n",
            "[2, 3] loss: 2.110\n",
            "[3, validation] loss: 2.203 accuracy: 19.633\n",
            "[3, 0] loss: 2.130\n",
            "[3, 1] loss: 2.191\n",
            "[3, 2] loss: 2.047\n",
            "[3, 3] loss: 1.746\n",
            "[4, validation] loss: 2.153 accuracy: 27.033\n",
            "[4, 0] loss: 2.165\n",
            "[4, 1] loss: 1.921\n",
            "[4, 2] loss: 2.012\n",
            "[4, 3] loss: 1.827\n",
            "[5, validation] loss: 2.088 accuracy: 37.433\n",
            "[5, 0] loss: 1.850\n",
            "[5, 1] loss: 2.065\n",
            "[5, 2] loss: 1.886\n",
            "[5, 3] loss: 2.456\n",
            "[6, validation] loss: 2.024 accuracy: 38.333\n",
            "[6, 0] loss: 2.012\n",
            "[6, 1] loss: 1.703\n",
            "[6, 2] loss: 1.926\n",
            "[6, 3] loss: 1.751\n",
            "[7, validation] loss: 1.937 accuracy: 38.100\n",
            "[7, 0] loss: 1.861\n",
            "[7, 1] loss: 1.754\n",
            "[7, 2] loss: 1.771\n",
            "[7, 3] loss: 2.007\n",
            "[8, validation] loss: 1.870 accuracy: 40.700\n",
            "[8, 0] loss: 1.778\n",
            "[8, 1] loss: 1.662\n",
            "[8, 2] loss: 1.766\n",
            "[8, 3] loss: 1.579\n",
            "[9, validation] loss: 1.812 accuracy: 50.167\n",
            "[9, 0] loss: 1.675\n",
            "[9, 1] loss: 1.666\n",
            "[9, 2] loss: 1.624\n",
            "[9, 3] loss: 1.943\n",
            "[10, validation] loss: 1.769 accuracy: 54.467\n",
            "[10, 0] loss: 1.628\n",
            "[10, 1] loss: 1.651\n",
            "[10, 2] loss: 1.505\n",
            "[10, 3] loss: 1.776\n",
            "[11, validation] loss: 1.710 accuracy: 64.633\n",
            "[11, 0] loss: 1.546\n",
            "[11, 1] loss: 1.555\n",
            "[11, 2] loss: 1.544\n",
            "[11, 3] loss: 1.429\n",
            "[12, validation] loss: 1.647 accuracy: 71.467\n",
            "[12, 0] loss: 1.437\n",
            "[12, 1] loss: 1.506\n",
            "[12, 2] loss: 1.547\n",
            "[12, 3] loss: 1.348\n",
            "[13, validation] loss: 1.600 accuracy: 70.733\n",
            "[13, 0] loss: 1.441\n",
            "[13, 1] loss: 1.512\n",
            "[13, 2] loss: 1.371\n",
            "[13, 3] loss: 1.254\n",
            "[14, validation] loss: 1.558 accuracy: 70.233\n",
            "[14, 0] loss: 1.327\n",
            "[14, 1] loss: 1.389\n",
            "[14, 2] loss: 1.415\n",
            "[14, 3] loss: 1.566\n",
            "[15, validation] loss: 1.518 accuracy: 71.500\n",
            "[15, 0] loss: 1.284\n",
            "[15, 1] loss: 1.354\n",
            "[15, 2] loss: 1.351\n",
            "[15, 3] loss: 1.276\n",
            "[16, validation] loss: 1.470 accuracy: 72.767\n",
            "[16, 0] loss: 1.254\n",
            "[16, 1] loss: 1.343\n",
            "[16, 2] loss: 1.247\n",
            "[16, 3] loss: 1.275\n",
            "[17, validation] loss: 1.431 accuracy: 73.167\n",
            "[17, 0] loss: 1.236\n",
            "[17, 1] loss: 1.225\n",
            "[17, 2] loss: 1.283\n",
            "[17, 3] loss: 1.328\n",
            "[18, validation] loss: 1.395 accuracy: 74.100\n",
            "[18, 0] loss: 1.163\n",
            "[18, 1] loss: 1.196\n",
            "[18, 2] loss: 1.237\n",
            "[18, 3] loss: 1.468\n",
            "[19, validation] loss: 1.350 accuracy: 75.833\n",
            "[19, 0] loss: 1.173\n",
            "[19, 1] loss: 1.257\n",
            "[19, 2] loss: 1.007\n",
            "[19, 3] loss: 1.455\n",
            "[20, validation] loss: 1.319 accuracy: 76.200\n",
            "[20, 0] loss: 1.059\n",
            "[20, 1] loss: 1.187\n",
            "[20, 2] loss: 1.070\n",
            "[20, 3] loss: 1.461\n",
            "[21, validation] loss: 1.302 accuracy: 76.867\n",
            "[21, 0] loss: 1.123\n",
            "[21, 1] loss: 1.067\n",
            "[21, 2] loss: 1.048\n",
            "[21, 3] loss: 1.423\n",
            "[22, validation] loss: 1.283 accuracy: 76.600\n",
            "[22, 0] loss: 1.121\n",
            "[22, 1] loss: 1.045\n",
            "[22, 2] loss: 1.068\n",
            "[22, 3] loss: 0.862\n",
            "[23, validation] loss: 1.260 accuracy: 77.067\n",
            "[23, 0] loss: 1.114\n",
            "[23, 1] loss: 0.839\n",
            "[23, 2] loss: 1.118\n",
            "[23, 3] loss: 1.221\n",
            "[24, validation] loss: 1.226 accuracy: 78.033\n",
            "[24, 0] loss: 1.018\n",
            "[24, 1] loss: 1.026\n",
            "[24, 2] loss: 0.961\n",
            "[24, 3] loss: 0.804\n",
            "[25, validation] loss: 1.208 accuracy: 77.533\n",
            "[25, 0] loss: 1.088\n",
            "[25, 1] loss: 0.896\n",
            "[25, 2] loss: 0.977\n",
            "[25, 3] loss: 0.704\n",
            "[26, validation] loss: 1.199 accuracy: 75.267\n",
            "[26, 0] loss: 1.035\n",
            "[26, 1] loss: 0.848\n",
            "[26, 2] loss: 0.979\n",
            "[26, 3] loss: 1.072\n",
            "[27, validation] loss: 1.170 accuracy: 75.733\n",
            "[27, 0] loss: 0.924\n",
            "[27, 1] loss: 1.027\n",
            "[27, 2] loss: 0.840\n",
            "[27, 3] loss: 0.896\n",
            "[28, validation] loss: 1.135 accuracy: 77.467\n",
            "[28, 0] loss: 0.871\n",
            "[28, 1] loss: 0.986\n",
            "[28, 2] loss: 0.840\n",
            "[28, 3] loss: 0.756\n",
            "[29, validation] loss: 1.112 accuracy: 78.567\n",
            "[29, 0] loss: 0.826\n",
            "[29, 1] loss: 0.886\n",
            "[29, 2] loss: 0.866\n",
            "[29, 3] loss: 1.167\n",
            "[30, validation] loss: 1.090 accuracy: 78.767\n",
            "[30, 0] loss: 0.748\n",
            "[30, 1] loss: 0.941\n",
            "[30, 2] loss: 0.823\n",
            "[30, 3] loss: 1.088\n",
            "[31, validation] loss: 1.055 accuracy: 80.800\n",
            "[31, 0] loss: 0.834\n",
            "[31, 1] loss: 0.840\n",
            "[31, 2] loss: 0.833\n",
            "[31, 3] loss: 0.633\n",
            "[32, validation] loss: 1.025 accuracy: 83.333\n",
            "[32, 0] loss: 0.806\n",
            "[32, 1] loss: 0.817\n",
            "[32, 2] loss: 0.812\n",
            "[32, 3] loss: 0.720\n",
            "[33, validation] loss: 1.006 accuracy: 83.233\n",
            "[33, 0] loss: 0.852\n",
            "[33, 1] loss: 0.737\n",
            "[33, 2] loss: 0.789\n",
            "[33, 3] loss: 0.882\n",
            "[34, validation] loss: 0.998 accuracy: 80.867\n",
            "[34, 0] loss: 0.813\n",
            "[34, 1] loss: 0.728\n",
            "[34, 2] loss: 0.768\n",
            "[34, 3] loss: 0.745\n",
            "[35, validation] loss: 0.989 accuracy: 79.600\n",
            "[35, 0] loss: 0.642\n",
            "[35, 1] loss: 0.687\n",
            "[35, 2] loss: 0.918\n",
            "[35, 3] loss: 0.840\n",
            "[36, validation] loss: 0.986 accuracy: 78.767\n",
            "[36, 0] loss: 0.670\n",
            "[36, 1] loss: 0.851\n",
            "[36, 2] loss: 0.684\n",
            "[36, 3] loss: 0.781\n",
            "[37, validation] loss: 0.980 accuracy: 78.467\n",
            "[37, 0] loss: 0.672\n",
            "[37, 1] loss: 0.744\n",
            "[37, 2] loss: 0.619\n",
            "[37, 3] loss: 1.755\n",
            "[38, validation] loss: 0.967 accuracy: 78.867\n",
            "[38, 0] loss: 0.732\n",
            "[38, 1] loss: 0.710\n",
            "[38, 2] loss: 0.705\n",
            "[38, 3] loss: 0.417\n",
            "[39, validation] loss: 0.940 accuracy: 80.333\n",
            "[39, 0] loss: 0.736\n",
            "[39, 1] loss: 0.669\n",
            "[39, 2] loss: 0.629\n",
            "[39, 3] loss: 0.921\n",
            "[40, validation] loss: 0.921 accuracy: 80.700\n",
            "[40, 0] loss: 0.571\n",
            "[40, 1] loss: 0.809\n",
            "[40, 2] loss: 0.668\n",
            "[40, 3] loss: 0.561\n",
            "[41, validation] loss: 0.906 accuracy: 81.267\n",
            "[41, 0] loss: 0.674\n",
            "[41, 1] loss: 0.549\n",
            "[41, 2] loss: 0.775\n",
            "[41, 3] loss: 0.645\n",
            "[42, validation] loss: 0.897 accuracy: 80.967\n",
            "[42, 0] loss: 0.688\n",
            "[42, 1] loss: 0.657\n",
            "[42, 2] loss: 0.602\n",
            "[42, 3] loss: 0.718\n",
            "[43, validation] loss: 0.893 accuracy: 80.100\n",
            "[43, 0] loss: 0.643\n",
            "[43, 1] loss: 0.662\n",
            "[43, 2] loss: 0.657\n",
            "[43, 3] loss: 0.374\n",
            "[44, validation] loss: 0.891 accuracy: 79.100\n",
            "[44, 0] loss: 0.548\n",
            "[44, 1] loss: 0.682\n",
            "[44, 2] loss: 0.658\n",
            "[44, 3] loss: 0.657\n",
            "[45, validation] loss: 0.875 accuracy: 80.167\n",
            "[45, 0] loss: 0.704\n",
            "[45, 1] loss: 0.608\n",
            "[45, 2] loss: 0.565\n",
            "[45, 3] loss: 0.266\n",
            "[46, validation] loss: 0.849 accuracy: 82.533\n",
            "[46, 0] loss: 0.588\n",
            "[46, 1] loss: 0.678\n",
            "[46, 2] loss: 0.550\n",
            "[46, 3] loss: 0.376\n",
            "[47, validation] loss: 0.835 accuracy: 82.933\n",
            "[47, 0] loss: 0.518\n",
            "[47, 1] loss: 0.739\n",
            "[47, 2] loss: 0.493\n",
            "[47, 3] loss: 0.674\n",
            "[48, validation] loss: 0.832 accuracy: 82.233\n",
            "[48, 0] loss: 0.604\n",
            "[48, 1] loss: 0.663\n",
            "[48, 2] loss: 0.431\n",
            "[48, 3] loss: 0.842\n",
            "[49, validation] loss: 0.831 accuracy: 81.600\n",
            "[49, 0] loss: 0.644\n",
            "[49, 1] loss: 0.522\n",
            "[49, 2] loss: 0.544\n",
            "[49, 3] loss: 0.574\n",
            "[50, validation] loss: 0.835 accuracy: 81.267\n",
            "[50, 0] loss: 0.646\n",
            "[50, 1] loss: 0.594\n",
            "[50, 2] loss: 0.483\n",
            "[50, 3] loss: 0.376\n",
            "[51, validation] loss: 0.832 accuracy: 81.467\n",
            "[51, 0] loss: 0.601\n",
            "[51, 1] loss: 0.550\n",
            "[51, 2] loss: 0.541\n",
            "[51, 3] loss: 0.416\n",
            "[52, validation] loss: 0.823 accuracy: 81.167\n",
            "[52, 0] loss: 0.501\n",
            "[52, 1] loss: 0.533\n",
            "[52, 2] loss: 0.609\n",
            "[52, 3] loss: 0.544\n",
            "[53, validation] loss: 0.811 accuracy: 80.800\n",
            "[53, 0] loss: 0.585\n",
            "[53, 1] loss: 0.500\n",
            "[53, 2] loss: 0.525\n",
            "[53, 3] loss: 0.484\n",
            "[54, validation] loss: 0.799 accuracy: 81.467\n",
            "[54, 0] loss: 0.469\n",
            "[54, 1] loss: 0.558\n",
            "[54, 2] loss: 0.557\n",
            "[54, 3] loss: 0.420\n",
            "[55, validation] loss: 0.794 accuracy: 81.100\n",
            "[55, 0] loss: 0.572\n",
            "[55, 1] loss: 0.412\n",
            "[55, 2] loss: 0.569\n",
            "[55, 3] loss: 0.562\n",
            "[56, validation] loss: 0.783 accuracy: 81.633\n",
            "[56, 0] loss: 0.432\n",
            "[56, 1] loss: 0.568\n",
            "[56, 2] loss: 0.516\n",
            "[56, 3] loss: 0.564\n",
            "[57, validation] loss: 0.772 accuracy: 82.267\n",
            "[57, 0] loss: 0.509\n",
            "[57, 1] loss: 0.488\n",
            "[57, 2] loss: 0.514\n",
            "[57, 3] loss: 0.452\n",
            "[58, validation] loss: 0.757 accuracy: 83.833\n",
            "[58, 0] loss: 0.549\n",
            "[58, 1] loss: 0.414\n",
            "[58, 2] loss: 0.490\n",
            "[58, 3] loss: 0.725\n",
            "[59, validation] loss: 0.751 accuracy: 84.533\n",
            "[59, 0] loss: 0.514\n",
            "[59, 1] loss: 0.502\n",
            "[59, 2] loss: 0.439\n",
            "[59, 3] loss: 0.638\n",
            "[60, validation] loss: 0.742 accuracy: 85.000\n",
            "[60, 0] loss: 0.462\n",
            "[60, 1] loss: 0.459\n",
            "[60, 2] loss: 0.525\n",
            "[60, 3] loss: 0.684\n",
            "[61, validation] loss: 0.737 accuracy: 85.033\n",
            "[61, 0] loss: 0.437\n",
            "[61, 1] loss: 0.433\n",
            "[61, 2] loss: 0.603\n",
            "[61, 3] loss: 0.449\n",
            "[62, validation] loss: 0.730 accuracy: 85.133\n",
            "[62, 0] loss: 0.355\n",
            "[62, 1] loss: 0.469\n",
            "[62, 2] loss: 0.613\n",
            "[62, 3] loss: 0.435\n",
            "[63, validation] loss: 0.719 accuracy: 85.467\n",
            "[63, 0] loss: 0.336\n",
            "[63, 1] loss: 0.509\n",
            "[63, 2] loss: 0.522\n",
            "[63, 3] loss: 0.606\n",
            "[64, validation] loss: 0.716 accuracy: 84.533\n",
            "[64, 0] loss: 0.438\n",
            "[64, 1] loss: 0.431\n",
            "[64, 2] loss: 0.471\n",
            "[64, 3] loss: 0.568\n",
            "[65, validation] loss: 0.729 accuracy: 82.567\n",
            "[65, 0] loss: 0.493\n",
            "[65, 1] loss: 0.467\n",
            "[65, 2] loss: 0.380\n",
            "[65, 3] loss: 0.406\n",
            "[66, validation] loss: 0.737 accuracy: 81.133\n",
            "[66, 0] loss: 0.504\n",
            "[66, 1] loss: 0.445\n",
            "[66, 2] loss: 0.387\n",
            "[66, 3] loss: 0.369\n",
            "[67, validation] loss: 0.731 accuracy: 81.433\n",
            "[67, 0] loss: 0.300\n",
            "[67, 1] loss: 0.546\n",
            "[67, 2] loss: 0.478\n",
            "[67, 3] loss: 0.343\n",
            "[68, validation] loss: 0.728 accuracy: 81.600\n",
            "[68, 0] loss: 0.389\n",
            "[68, 1] loss: 0.504\n",
            "[68, 2] loss: 0.392\n",
            "[68, 3] loss: 0.495\n",
            "[69, validation] loss: 0.715 accuracy: 82.033\n",
            "[69, 0] loss: 0.400\n",
            "[69, 1] loss: 0.490\n",
            "[69, 2] loss: 0.372\n",
            "[69, 3] loss: 0.528\n",
            "[70, validation] loss: 0.693 accuracy: 84.000\n",
            "[70, 0] loss: 0.433\n",
            "[70, 1] loss: 0.396\n",
            "[70, 2] loss: 0.414\n",
            "[70, 3] loss: 0.480\n",
            "[71, validation] loss: 0.683 accuracy: 85.300\n",
            "[71, 0] loss: 0.399\n",
            "[71, 1] loss: 0.386\n",
            "[71, 2] loss: 0.472\n",
            "[71, 3] loss: 0.223\n",
            "[72, validation] loss: 0.674 accuracy: 85.433\n",
            "[72, 0] loss: 0.401\n",
            "[72, 1] loss: 0.312\n",
            "[72, 2] loss: 0.493\n",
            "[72, 3] loss: 0.531\n",
            "[73, validation] loss: 0.666 accuracy: 85.467\n",
            "[73, 0] loss: 0.404\n",
            "[73, 1] loss: 0.338\n",
            "[73, 2] loss: 0.430\n",
            "[73, 3] loss: 0.648\n",
            "[74, validation] loss: 0.657 accuracy: 86.067\n",
            "[74, 0] loss: 0.377\n",
            "[74, 1] loss: 0.382\n",
            "[74, 2] loss: 0.409\n",
            "[74, 3] loss: 0.488\n",
            "[75, validation] loss: 0.656 accuracy: 85.467\n",
            "[75, 0] loss: 0.361\n",
            "[75, 1] loss: 0.382\n",
            "[75, 2] loss: 0.380\n",
            "[75, 3] loss: 0.702\n",
            "[76, validation] loss: 0.657 accuracy: 84.933\n",
            "[76, 0] loss: 0.337\n",
            "[76, 1] loss: 0.376\n",
            "[76, 2] loss: 0.464\n",
            "[76, 3] loss: 0.222\n",
            "[77, validation] loss: 0.661 accuracy: 84.333\n",
            "[77, 0] loss: 0.348\n",
            "[77, 1] loss: 0.371\n",
            "[77, 2] loss: 0.409\n",
            "[77, 3] loss: 0.579\n",
            "[78, validation] loss: 0.659 accuracy: 84.000\n",
            "[78, 0] loss: 0.350\n",
            "[78, 1] loss: 0.366\n",
            "[78, 2] loss: 0.396\n",
            "[78, 3] loss: 0.632\n",
            "[79, validation] loss: 0.647 accuracy: 85.067\n",
            "[79, 0] loss: 0.343\n",
            "[79, 1] loss: 0.356\n",
            "[79, 2] loss: 0.397\n",
            "[79, 3] loss: 0.486\n",
            "[80, validation] loss: 0.642 accuracy: 85.200\n",
            "[80, 0] loss: 0.324\n",
            "[80, 1] loss: 0.387\n",
            "[80, 2] loss: 0.414\n",
            "[80, 3] loss: 0.275\n",
            "[81, validation] loss: 0.645 accuracy: 84.967\n",
            "[81, 0] loss: 0.345\n",
            "[81, 1] loss: 0.353\n",
            "[81, 2] loss: 0.421\n",
            "[81, 3] loss: 0.436\n",
            "[82, validation] loss: 0.648 accuracy: 84.700\n",
            "[82, 0] loss: 0.286\n",
            "[82, 1] loss: 0.346\n",
            "[82, 2] loss: 0.474\n",
            "[82, 3] loss: 0.471\n",
            "[83, validation] loss: 0.641 accuracy: 85.300\n",
            "[83, 0] loss: 0.386\n",
            "[83, 1] loss: 0.396\n",
            "[83, 2] loss: 0.283\n",
            "[83, 3] loss: 0.571\n",
            "[84, validation] loss: 0.626 accuracy: 85.800\n",
            "[84, 0] loss: 0.320\n",
            "[84, 1] loss: 0.405\n",
            "[84, 2] loss: 0.338\n",
            "[84, 3] loss: 0.289\n",
            "[85, validation] loss: 0.621 accuracy: 86.267\n",
            "[85, 0] loss: 0.364\n",
            "[85, 1] loss: 0.418\n",
            "[85, 2] loss: 0.280\n",
            "[85, 3] loss: 0.157\n",
            "[86, validation] loss: 0.617 accuracy: 86.333\n",
            "[86, 0] loss: 0.316\n",
            "[86, 1] loss: 0.334\n",
            "[86, 2] loss: 0.379\n",
            "[86, 3] loss: 0.314\n",
            "[87, validation] loss: 0.617 accuracy: 85.867\n",
            "[87, 0] loss: 0.414\n",
            "[87, 1] loss: 0.326\n",
            "[87, 2] loss: 0.281\n",
            "[87, 3] loss: 0.298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3g7sNGZChB9G",
        "colab": {}
      },
      "source": [
        "plt.plot(training_loss, label='training loss')\n",
        "plt.plot(np.arange(len(validation_loss)) * len(training_subset) / BATCH_SIZE, validation_loss, label='validation loss')\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8HSHSVvRmjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataloader = torch.utils.data.DataLoader(mnist_test,\n",
        "                                             batch_size=1,\n",
        "                                             num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2dNc7aeap_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classification_test(classifier, autoencoder.encoder, test_dataloader, cross_entropy, epoch, [])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}