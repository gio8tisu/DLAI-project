{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaWaYXG1fIRm",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GA4Aiz2Giu5e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'batch_size':32,\n",
    "    'num_epochs':15,\n",
    "    'num_workers': 4,\n",
    "    'train_percentage': 0.95,\n",
    "    'num_classes':10,\n",
    "    'num_inputs':784,\n",
    "    'learning_rate':1e-3,\n",
    "    'log_interval':500,\n",
    "}\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyK3i7pS9gSH"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Vf3xEV29k0y"
   },
   "source": [
    "## Module definition\n",
    "\n",
    "Our autoencoder will be defined by the `ConvolutionalAutoencoder` class which uses a `ConvolutionalEncoder` object to encode followed a `ConvolutionalDecoder` object to decode. The `ConvolutionalEncoder` and `ConvolutionalDecoder` classes make use of `n_blocks` `ConvolutionalBlock`s or `DeconvolutionalBlock`s which are composed of `layer_per_block` convolution layers with the same number of filters.\n",
    "\n",
    "The dimensionality is reduced by applying 2-factor spatial downsampling at each block. The number of filters is doubled for each subsequent block. The decoder makes the exact oposite process.\n",
    "\n",
    "The final layer uses a tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6ZgDF0kfIRy",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_blocks, downsampling_method, upsampling_method,\n",
    "                 layers_per_block=2, bottleneck_size=16):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.downsampling_method = downsampling_method\n",
    "        self.upsampling_method = upsampling_method\n",
    "\n",
    "        self.encoder = ConvolutionalEncoder(n_blocks, downsampling_method,\n",
    "                                            layers_per_block=layers_per_block, bottleneck_size=bottleneck_size)\n",
    "        self.decoder = ConvolutionalDecoder(n_blocks, upsampling_method,\n",
    "                                            self.encoder.output_channels,\n",
    "                                            layers_per_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        reconstruction = self.decoder(code)\n",
    "        return reconstruction\n",
    "\n",
    "\n",
    "class ConvolutionalEncoder(torch.nn.Module):\n",
    "\n",
    "    DOWNSAMPLING_METHODS = [\"max-pooling\", \"avg-pooling\", \"stride-2\"]\n",
    "\n",
    "    def __init__(self, n_blocks, downsampling_method, init_filters=16,\n",
    "                 layers_per_block=2, kernel_size=5, input_channels=1, bottleneck_size=16):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        assert downsampling_method in self.DOWNSAMPLING_METHODS\n",
    "        self.downsampling_method = downsampling_method\n",
    "        self.layers_per_block = layers_per_block\n",
    "        self.init_filters = init_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # First layer so we have <init_filters> channels.\n",
    "        n_filters = init_filters\n",
    "        layers.append(\n",
    "            ConvolutionalBlock(input_channels, n_filters, kernel_size, 1))\n",
    "\n",
    "        # Encoding blocks.\n",
    "        input_channels = n_filters\n",
    "        for _ in range(n_blocks-1):\n",
    "            if downsampling_method == \"max-pooling\":\n",
    "                # Convolutional block + max pooling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, n_filters, kernel_size, layers_per_block),\n",
    "                    torch.nn.MaxPool2d(2)\n",
    "                )                    \n",
    "            elif downsampling_method == \"avg-pooling\":\n",
    "                # Convolutional block + average pooling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, n_filters, kernel_size, layers_per_block),\n",
    "                    torch.nn.AvgPool2d(2)\n",
    "                )\n",
    "            else:\n",
    "                # Stride-2 convolution.\n",
    "                conv_block = ConvolutionalBlock(input_channels, n_filters,\n",
    "                                                kernel_size,\n",
    "                                                layers_per_block,\n",
    "                                                last_stride=2)\n",
    "            \n",
    "            layers.append(conv_block)\n",
    "            # Double the number of filters.\n",
    "            input_channels = n_filters\n",
    "            n_filters = 2 * n_filters\n",
    "            \n",
    "        #Bottleneck\n",
    "        if downsampling_method == \"max-pooling\":\n",
    "                # Convolutional block + max pooling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, bottleneck_size, kernel_size, layers_per_block),\n",
    "                    torch.nn.MaxPool2d(2)\n",
    "                )                    \n",
    "        elif downsampling_method == \"avg-pooling\":\n",
    "            # Convolutional block + average pooling.\n",
    "            conv_block = torch.nn.Sequential(\n",
    "                ConvolutionalBlock(input_channels, bottleneck_size, kernel_size, layers_per_block),\n",
    "                torch.nn.AvgPool2d(2)\n",
    "            )\n",
    "        else:\n",
    "            # Stride-2 convolution.\n",
    "            conv_block = ConvolutionalBlock(input_channels, bottleneck_size,\n",
    "                                            kernel_size,\n",
    "                                            layers_per_block,\n",
    "                                            last_stride=2)\n",
    "            \n",
    "        layers.append(conv_block) #the last layer of the encoder is the bottleneck        \n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*layers)\n",
    "        self.output_channels = bottleneck_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "class ConvolutionalDecoder(torch.nn.Module):\n",
    "\n",
    "    UPSAMPLING_METHODS = [\"transposed\", \"bilinear\", \"bicubic\", \"nearest\"]\n",
    "\n",
    "    def __init__(self, n_blocks, upsampling_method, input_channels,\n",
    "                 layers_per_block=2, kernel_size=5, output_channels=1):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        assert upsampling_method in self.UPSAMPLING_METHODS\n",
    "        self.upsampling_method = upsampling_method\n",
    "        self.layers_per_block = layers_per_block\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Decoding blocks.\n",
    "        n_filters = input_channels\n",
    "        for _ in range(n_blocks):\n",
    "            if upsampling_method == \"transposed\":\n",
    "                # Deconvolutional block\n",
    "                conv_block = DeconvolutionalBlock(input_channels, n_filters,\n",
    "                                                  kernel_size, layers_per_block,\n",
    "                                                  stride=2)\n",
    "            else:\n",
    "                # Upsampling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, n_filters, kernel_size,\n",
    "                                       layers_per_block),\n",
    "                    torch.nn.Upsample(scale_factor=2, mode=upsampling_method)\n",
    "                )\n",
    "            layers.append(conv_block)\n",
    "            # Half the number of filters.\n",
    "            input_channels = n_filters\n",
    "            n_filters = n_filters // 2\n",
    "\n",
    "        # Last layer so we have <output_channels> channel.\n",
    "        layers.append(torch.nn.Conv2d(input_channels, output_channels, kernel_size,\n",
    "                                      padding=kernel_size // 2))\n",
    "        layers.append(torch.nn.Tanh())\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class ConvolutionalBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Applies n_layers convolutional layers with the same number of\n",
    "    filters (n_filters) and filter sizes (kerne_size) with ReLU activations\n",
    "    keeping the same spacial size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, n_filters, kernel_size, n_layers,\n",
    "                 last_stride=1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        padding = kernel_size // 2  # To keep the same size.\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:  # First layer with correct input channels.\n",
    "                layers.append(torch.nn.Conv2d(input_channels, n_filters,\n",
    "                                              kernel_size, padding=padding))\n",
    "            elif 0 < i < n_layers:  # Intermediate layers.\n",
    "                layers.append(torch.nn.Conv2d(n_filters, n_filters,\n",
    "                                              kernel_size, padding=padding))\n",
    "            else:  # Last layer with stride.\n",
    "                layers.append(torch.nn.Conv2d(n_filters, n_filters,\n",
    "                                              kernel_size, last_stride, padding))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # To sequentially apply the layers.\n",
    "        self.block = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class DeconvolutionalBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Applies a transposed convolution followed by n_layers-1 convolutional\n",
    "    layers with the same number of filters and filter sizes with ReLU\n",
    "    activations keeping the same spacial size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, n_filters, kernel_size, n_layers, stride):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Transposed convolution layer.\n",
    "        layers.append(torch.nn.ConvTranspose2d(input_channels, n_filters,\n",
    "                                               kernel_size, stride, padding))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(torch.nn.Conv2d(n_filters, n_filters, kernel_size,\n",
    "                                          padding=padding))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # To sequentially apply the layers.\n",
    "        self.block = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_ZD0rgWBhJa"
   },
   "source": [
    "To do a quick test, we will pass a random image and check if the output is of the same size as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIO0AZWCfIR2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "image = torch.randn((1, 1, 128, 128))\n",
    "autoencoder = ConvolutionalAutoencoder(2, 'max-pooling', 'nearest').to(device)\n",
    "output = autoencoder(image.to(device))\n",
    "assert output.shape == (1, 1, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z25DsijeCNvi"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We split the training set and normalize the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObQndlPufIR9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "     torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.1307,), (0.3081,))  # Mean and std from internet...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11325,
     "status": "ok",
     "timestamp": 1575228638843,
     "user": {
      "displayName": "Sergio Garcia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDTm-mE3fT-h7wpEZpGtZu9jSK24Hqhb2nqpKiyysE=s64",
      "userId": "16139820604268433723"
     },
     "user_tz": -60
    },
    "id": "1VIGvzG4fISD",
    "outputId": "3bc1ae58-6e5f-4a3f-8e2e-f678fba722af",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.MNIST('mnist_dataset', train=True, transform=transform, download=True)\n",
    "dataset_len = len(mnist)\n",
    "train_size = int(hparams[\"train_percentage\"] * dataset_len)\n",
    "val_size = int((1 - hparams[\"train_percentage\"]) * dataset_len)\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5yH0O1kfISY",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(mnist_train,\n",
    "                                               batch_size=hparams[\"batch_size\"],\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=hparams[\"num_workers\"])\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(mnist_val,\n",
    "                                               batch_size=hparams[\"batch_size\"],\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=hparams[\"num_workers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section defines the loss and the optimizer. In this case we will use Mean Squared Error loss as we are making the reconstruction of the image. The optimizer used is Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNNQ25Z-fISe",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mse = F.mse_loss\n",
    "adam = torch.optim.Adam(autoencoder.parameters(), lr=hparams[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEwsot_KfISi",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, epochs):\n",
    "    train_loss_hist, val_loss_hist = [], []\n",
    "    exp_weight = 0.1 #for exponential weighted avg of the loss\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Starting epoch number {epoch+1}\")\n",
    "        model.train()\n",
    "        avg_loss = None        \n",
    "        \n",
    "        #Train\n",
    "        for batch_index, (data, _) in enumerate(train_dataloader):            \n",
    "            images = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(images)\n",
    "            loss = criterion(reconstructed, images, reduction=\"sum\") / images.shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if avg_loss:\n",
    "                avg_loss = exp_weight * loss.item() + (1 - exp_weight) * avg_loss\n",
    "            else:\n",
    "                avg_loss = loss.item()\n",
    "\n",
    "            if batch_index % hparams[\"log_interval\"] == 0:\n",
    "                print('[Epoch %d, Batch %d] TRAIN loss: %.3f' %\n",
    "                      (epoch + 1, batch_index + 1, avg_loss))\n",
    "                      \n",
    "        train_loss_hist.append(avg_loss)    \n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (data, _) in enumerate(val_dataloader):\n",
    "                images = data.to(device)\n",
    "                reconstructed = model(images)\n",
    "                val_loss += criterion(reconstructed, images, reduction=\"sum\").item()\n",
    "        # Average the val loss across all elements\n",
    "        val_loss /= len(val_dataloader.dataset)\n",
    "        val_loss_hist.append(val_loss)\n",
    "        print('[Epoch %d] VALIDATION loss: %.3f' %\n",
    "              (epoch + 1, val_loss))\n",
    "    return train_loss_hist, val_loss_hist\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91626,
     "status": "ok",
     "timestamp": 1575228794730,
     "user": {
      "displayName": "Sergio Garcia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDTm-mE3fT-h7wpEZpGtZu9jSK24Hqhb2nqpKiyysE=s64",
      "userId": "16139820604268433723"
     },
     "user_tz": -60
    },
    "id": "CXQlTiq2fISm",
    "outputId": "4c8762a1-882e-4092-ae1d-d01a899bc9c0",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Starting epoch number 1\n",
      "[Epoch 1, Batch 1] TRAIN loss: 381.905\n",
      "[Epoch 1, Batch 501] TRAIN loss: 276.716\n",
      "[Epoch 1, Batch 1001] TRAIN loss: 270.852\n",
      "[Epoch 1, Batch 1501] TRAIN loss: 265.535\n",
      "[Epoch 1] VALIDATION loss: 265.709\n",
      "==============================\n",
      "Starting epoch number 2\n",
      "[Epoch 2, Batch 1] TRAIN loss: 265.509\n",
      "[Epoch 2, Batch 501] TRAIN loss: 259.517\n",
      "[Epoch 2, Batch 1001] TRAIN loss: 262.893\n",
      "[Epoch 2, Batch 1501] TRAIN loss: 265.070\n",
      "[Epoch 2] VALIDATION loss: 261.775\n",
      "==============================\n",
      "Starting epoch number 3\n",
      "[Epoch 3, Batch 1] TRAIN loss: 273.603\n",
      "[Epoch 3, Batch 501] TRAIN loss: 257.398\n",
      "[Epoch 3, Batch 1001] TRAIN loss: 268.325\n",
      "[Epoch 3, Batch 1501] TRAIN loss: 265.700\n",
      "[Epoch 3] VALIDATION loss: 258.796\n",
      "==============================\n",
      "Starting epoch number 4\n",
      "[Epoch 4, Batch 1] TRAIN loss: 286.785\n",
      "[Epoch 4, Batch 501] TRAIN loss: 264.240\n",
      "[Epoch 4, Batch 1001] TRAIN loss: 263.002\n",
      "[Epoch 4, Batch 1501] TRAIN loss: 254.371\n",
      "[Epoch 4] VALIDATION loss: 257.224\n",
      "==============================\n",
      "Starting epoch number 5\n",
      "[Epoch 5, Batch 1] TRAIN loss: 270.949\n",
      "[Epoch 5, Batch 501] TRAIN loss: 252.277\n",
      "[Epoch 5, Batch 1001] TRAIN loss: 253.645\n",
      "[Epoch 5, Batch 1501] TRAIN loss: 263.784\n",
      "[Epoch 5] VALIDATION loss: 255.975\n"
     ]
    }
   ],
   "source": [
    "train_loss_history, val_loss_history = train(autoencoder, train_dataloader, val_dataloader, mse, adam, hparams[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the plot of the loss curves during train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OKrrQKRfISs",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(hparams[\"num_epochs\"])\n",
    "plt.title('Autoencoder reconstruction loss curves')\n",
    "plt.plot(epochs, train_loss_history, label='Training loss')\n",
    "plt.plot(epochs, val_loss_history, '-x', label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyEnlw7JiJ-X",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "Reconstructed image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image):\n",
    "    plt.subplot(121).set_title('Original image')\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.subplot(122).set_title('Reconstructed image')\n",
    "    plt.imshow(reconstructed[0][0].detach().cpu().numpy(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_test = torchvision.datasets.MNIST('mnist_dataset', train=False, transform=transform)\n",
    "reconstructed = autoencoder(mnist_test[6][0].to(device).unsqueeze(0))\n",
    "show_image(reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with different types of bottlenecks sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Autoencoder bottleneck size 4***************\n",
      "==============================\n",
      "Starting epoch number 1\n",
      "[Epoch 1, Batch 1] TRAIN loss: 754.279\n",
      "[Epoch 1, Batch 501] TRAIN loss: 795.454\n",
      "[Epoch 1, Batch 1001] TRAIN loss: 811.119\n",
      "[Epoch 1, Batch 1501] TRAIN loss: 802.000\n",
      "[Epoch 1] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 2\n",
      "[Epoch 2, Batch 1] TRAIN loss: 719.090\n",
      "[Epoch 2, Batch 501] TRAIN loss: 798.181\n",
      "[Epoch 2, Batch 1001] TRAIN loss: 813.706\n",
      "[Epoch 2, Batch 1501] TRAIN loss: 798.601\n",
      "[Epoch 2] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 3\n",
      "[Epoch 3, Batch 1] TRAIN loss: 868.788\n",
      "[Epoch 3, Batch 501] TRAIN loss: 803.615\n",
      "[Epoch 3, Batch 1001] TRAIN loss: 778.830\n",
      "[Epoch 3, Batch 1501] TRAIN loss: 819.112\n",
      "[Epoch 3] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 4\n",
      "[Epoch 4, Batch 1] TRAIN loss: 812.685\n",
      "[Epoch 4, Batch 501] TRAIN loss: 795.353\n",
      "[Epoch 4, Batch 1001] TRAIN loss: 793.992\n",
      "[Epoch 4, Batch 1501] TRAIN loss: 797.278\n",
      "[Epoch 4] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 5\n",
      "[Epoch 5, Batch 1] TRAIN loss: 903.496\n",
      "[Epoch 5, Batch 501] TRAIN loss: 802.726\n",
      "[Epoch 5, Batch 1001] TRAIN loss: 826.172\n",
      "[Epoch 5, Batch 1501] TRAIN loss: 791.516\n",
      "[Epoch 5] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 6\n",
      "[Epoch 6, Batch 1] TRAIN loss: 807.993\n",
      "[Epoch 6, Batch 501] TRAIN loss: 823.398\n",
      "[Epoch 6, Batch 1001] TRAIN loss: 816.723\n",
      "[Epoch 6, Batch 1501] TRAIN loss: 788.565\n",
      "[Epoch 6] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 7\n",
      "[Epoch 7, Batch 1] TRAIN loss: 762.506\n",
      "[Epoch 7, Batch 501] TRAIN loss: 812.927\n",
      "[Epoch 7, Batch 1001] TRAIN loss: 783.809\n",
      "[Epoch 7, Batch 1501] TRAIN loss: 798.131\n",
      "[Epoch 7] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 8\n",
      "[Epoch 8, Batch 1] TRAIN loss: 857.965\n",
      "[Epoch 8, Batch 501] TRAIN loss: 809.113\n",
      "[Epoch 8, Batch 1001] TRAIN loss: 786.522\n",
      "[Epoch 8, Batch 1501] TRAIN loss: 791.527\n",
      "[Epoch 8] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 9\n",
      "[Epoch 9, Batch 1] TRAIN loss: 792.526\n",
      "[Epoch 9, Batch 501] TRAIN loss: 800.476\n",
      "[Epoch 9, Batch 1001] TRAIN loss: 781.529\n",
      "[Epoch 9, Batch 1501] TRAIN loss: 803.054\n",
      "[Epoch 9] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 10\n",
      "[Epoch 10, Batch 1] TRAIN loss: 839.364\n",
      "[Epoch 10, Batch 501] TRAIN loss: 802.268\n",
      "[Epoch 10, Batch 1001] TRAIN loss: 819.903\n",
      "[Epoch 10, Batch 1501] TRAIN loss: 808.355\n",
      "[Epoch 10] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 11\n",
      "[Epoch 11, Batch 1] TRAIN loss: 797.545\n",
      "[Epoch 11, Batch 501] TRAIN loss: 799.903\n",
      "[Epoch 11, Batch 1001] TRAIN loss: 786.266\n",
      "[Epoch 11, Batch 1501] TRAIN loss: 794.472\n",
      "[Epoch 11] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 12\n",
      "[Epoch 12, Batch 1] TRAIN loss: 790.144\n",
      "[Epoch 12, Batch 501] TRAIN loss: 804.482\n",
      "[Epoch 12, Batch 1001] TRAIN loss: 804.157\n",
      "[Epoch 12, Batch 1501] TRAIN loss: 813.346\n",
      "[Epoch 12] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 13\n",
      "[Epoch 13, Batch 1] TRAIN loss: 805.172\n",
      "[Epoch 13, Batch 501] TRAIN loss: 785.596\n",
      "[Epoch 13, Batch 1001] TRAIN loss: 808.204\n",
      "[Epoch 13, Batch 1501] TRAIN loss: 802.617\n",
      "[Epoch 13] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 14\n",
      "[Epoch 14, Batch 1] TRAIN loss: 736.899\n",
      "[Epoch 14, Batch 501] TRAIN loss: 789.556\n",
      "[Epoch 14, Batch 1001] TRAIN loss: 803.512\n",
      "[Epoch 14, Batch 1501] TRAIN loss: 799.412\n",
      "[Epoch 14] VALIDATION loss: 796.526\n",
      "==============================\n",
      "Starting epoch number 15\n",
      "[Epoch 15, Batch 1] TRAIN loss: 802.717\n",
      "[Epoch 15, Batch 501] TRAIN loss: 802.212\n",
      "[Epoch 15, Batch 1001] TRAIN loss: 797.449\n",
      "[Epoch 15, Batch 1501] TRAIN loss: 816.873\n",
      "[Epoch 15] VALIDATION loss: 796.526\n",
      "***************Autoencoder bottleneck size 8***************\n",
      "==============================\n",
      "Starting epoch number 1\n",
      "[Epoch 1, Batch 1] TRAIN loss: 771.723\n",
      "[Epoch 1, Batch 501] TRAIN loss: 792.843\n",
      "[Epoch 1, Batch 1001] TRAIN loss: 784.154\n",
      "[Epoch 1, Batch 1501] TRAIN loss: 788.228\n",
      "[Epoch 1] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 2\n",
      "[Epoch 2, Batch 1] TRAIN loss: 831.020\n",
      "[Epoch 2, Batch 501] TRAIN loss: 781.576\n",
      "[Epoch 2, Batch 1001] TRAIN loss: 788.924\n",
      "[Epoch 2, Batch 1501] TRAIN loss: 782.307\n",
      "[Epoch 2] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 3\n",
      "[Epoch 3, Batch 1] TRAIN loss: 806.657\n",
      "[Epoch 3, Batch 501] TRAIN loss: 787.295\n",
      "[Epoch 3, Batch 1001] TRAIN loss: 787.277\n",
      "[Epoch 3, Batch 1501] TRAIN loss: 767.145\n",
      "[Epoch 3] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 4\n",
      "[Epoch 4, Batch 1] TRAIN loss: 781.804\n",
      "[Epoch 4, Batch 501] TRAIN loss: 786.668\n",
      "[Epoch 4, Batch 1001] TRAIN loss: 794.150\n",
      "[Epoch 4, Batch 1501] TRAIN loss: 783.151\n",
      "[Epoch 4] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 5\n",
      "[Epoch 5, Batch 1] TRAIN loss: 741.253\n",
      "[Epoch 5, Batch 501] TRAIN loss: 775.882\n",
      "[Epoch 5, Batch 1001] TRAIN loss: 785.758\n",
      "[Epoch 5, Batch 1501] TRAIN loss: 801.170\n",
      "[Epoch 5] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 6\n",
      "[Epoch 6, Batch 1] TRAIN loss: 824.222\n",
      "[Epoch 6, Batch 501] TRAIN loss: 773.960\n",
      "[Epoch 6, Batch 1001] TRAIN loss: 790.254\n",
      "[Epoch 6, Batch 1501] TRAIN loss: 787.829\n",
      "[Epoch 6] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 7\n",
      "[Epoch 7, Batch 1] TRAIN loss: 848.309\n",
      "[Epoch 7, Batch 501] TRAIN loss: 789.447\n",
      "[Epoch 7, Batch 1001] TRAIN loss: 796.965\n",
      "[Epoch 7, Batch 1501] TRAIN loss: 800.542\n",
      "[Epoch 7] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 8\n",
      "[Epoch 8, Batch 1] TRAIN loss: 794.655\n",
      "[Epoch 8, Batch 501] TRAIN loss: 774.177\n",
      "[Epoch 8, Batch 1001] TRAIN loss: 784.157\n",
      "[Epoch 8, Batch 1501] TRAIN loss: 772.019\n",
      "[Epoch 8] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 9\n",
      "[Epoch 9, Batch 1] TRAIN loss: 811.540\n",
      "[Epoch 9, Batch 501] TRAIN loss: 770.583\n",
      "[Epoch 9, Batch 1001] TRAIN loss: 783.359\n",
      "[Epoch 9, Batch 1501] TRAIN loss: 802.814\n",
      "[Epoch 9] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 10\n",
      "[Epoch 10, Batch 1] TRAIN loss: 757.938\n",
      "[Epoch 10, Batch 501] TRAIN loss: 797.343\n",
      "[Epoch 10, Batch 1001] TRAIN loss: 774.755\n",
      "[Epoch 10, Batch 1501] TRAIN loss: 790.975\n",
      "[Epoch 10] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 11\n",
      "[Epoch 11, Batch 1] TRAIN loss: 746.044\n",
      "[Epoch 11, Batch 501] TRAIN loss: 782.724\n",
      "[Epoch 11, Batch 1001] TRAIN loss: 782.936\n",
      "[Epoch 11, Batch 1501] TRAIN loss: 793.191\n",
      "[Epoch 11] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 12\n",
      "[Epoch 12, Batch 1] TRAIN loss: 793.618\n",
      "[Epoch 12, Batch 501] TRAIN loss: 808.250\n",
      "[Epoch 12, Batch 1001] TRAIN loss: 769.891\n",
      "[Epoch 12, Batch 1501] TRAIN loss: 778.678\n",
      "[Epoch 12] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 13\n",
      "[Epoch 13, Batch 1] TRAIN loss: 824.170\n",
      "[Epoch 13, Batch 501] TRAIN loss: 778.845\n",
      "[Epoch 13, Batch 1001] TRAIN loss: 786.132\n",
      "[Epoch 13, Batch 1501] TRAIN loss: 791.777\n",
      "[Epoch 13] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 14\n",
      "[Epoch 14, Batch 1] TRAIN loss: 749.352\n",
      "[Epoch 14, Batch 501] TRAIN loss: 783.675\n",
      "[Epoch 14, Batch 1001] TRAIN loss: 792.296\n",
      "[Epoch 14, Batch 1501] TRAIN loss: 787.799\n",
      "[Epoch 14] VALIDATION loss: 782.847\n",
      "==============================\n",
      "Starting epoch number 15\n",
      "[Epoch 15, Batch 1] TRAIN loss: 838.476\n",
      "[Epoch 15, Batch 501] TRAIN loss: 791.759\n",
      "[Epoch 15, Batch 1001] TRAIN loss: 799.599\n",
      "[Epoch 15, Batch 1501] TRAIN loss: 781.673\n",
      "[Epoch 15] VALIDATION loss: 782.847\n",
      "***************Autoencoder bottleneck size 12***************\n",
      "==============================\n",
      "Starting epoch number 1\n",
      "[Epoch 1, Batch 1] TRAIN loss: 818.101\n",
      "[Epoch 1, Batch 501] TRAIN loss: 766.419\n",
      "[Epoch 1, Batch 1001] TRAIN loss: 778.742\n",
      "[Epoch 1, Batch 1501] TRAIN loss: 774.518\n",
      "[Epoch 1] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 2\n",
      "[Epoch 2, Batch 1] TRAIN loss: 775.676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2, Batch 501] TRAIN loss: 792.770\n",
      "[Epoch 2, Batch 1001] TRAIN loss: 784.937\n",
      "[Epoch 2, Batch 1501] TRAIN loss: 780.204\n",
      "[Epoch 2] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 3\n",
      "[Epoch 3, Batch 1] TRAIN loss: 791.926\n",
      "[Epoch 3, Batch 501] TRAIN loss: 780.427\n",
      "[Epoch 3, Batch 1001] TRAIN loss: 784.379\n",
      "[Epoch 3, Batch 1501] TRAIN loss: 800.127\n",
      "[Epoch 3] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 4\n",
      "[Epoch 4, Batch 1] TRAIN loss: 885.269\n",
      "[Epoch 4, Batch 501] TRAIN loss: 790.982\n",
      "[Epoch 4, Batch 1001] TRAIN loss: 794.081\n",
      "[Epoch 4, Batch 1501] TRAIN loss: 778.313\n",
      "[Epoch 4] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 5\n",
      "[Epoch 5, Batch 1] TRAIN loss: 767.891\n",
      "[Epoch 5, Batch 501] TRAIN loss: 798.200\n",
      "[Epoch 5, Batch 1001] TRAIN loss: 795.288\n",
      "[Epoch 5, Batch 1501] TRAIN loss: 788.522\n",
      "[Epoch 5] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 6\n",
      "[Epoch 6, Batch 1] TRAIN loss: 800.296\n",
      "[Epoch 6, Batch 501] TRAIN loss: 771.698\n",
      "[Epoch 6, Batch 1001] TRAIN loss: 771.569\n",
      "[Epoch 6, Batch 1501] TRAIN loss: 786.408\n",
      "[Epoch 6] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 7\n",
      "[Epoch 7, Batch 1] TRAIN loss: 816.323\n",
      "[Epoch 7, Batch 501] TRAIN loss: 783.047\n",
      "[Epoch 7, Batch 1001] TRAIN loss: 792.068\n",
      "[Epoch 7, Batch 1501] TRAIN loss: 789.895\n",
      "[Epoch 7] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 8\n",
      "[Epoch 8, Batch 1] TRAIN loss: 738.941\n",
      "[Epoch 8, Batch 501] TRAIN loss: 792.208\n",
      "[Epoch 8, Batch 1001] TRAIN loss: 795.861\n",
      "[Epoch 8, Batch 1501] TRAIN loss: 787.592\n",
      "[Epoch 8] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 9\n",
      "[Epoch 9, Batch 1] TRAIN loss: 828.115\n",
      "[Epoch 9, Batch 501] TRAIN loss: 781.361\n",
      "[Epoch 9, Batch 1001] TRAIN loss: 787.617\n",
      "[Epoch 9, Batch 1501] TRAIN loss: 782.908\n",
      "[Epoch 9] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 10\n",
      "[Epoch 10, Batch 1] TRAIN loss: 866.501\n",
      "[Epoch 10, Batch 501] TRAIN loss: 789.727\n",
      "[Epoch 10, Batch 1001] TRAIN loss: 784.325\n",
      "[Epoch 10, Batch 1501] TRAIN loss: 806.540\n",
      "[Epoch 10] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 11\n",
      "[Epoch 11, Batch 1] TRAIN loss: 836.822\n",
      "[Epoch 11, Batch 501] TRAIN loss: 783.348\n",
      "[Epoch 11, Batch 1001] TRAIN loss: 779.934\n",
      "[Epoch 11, Batch 1501] TRAIN loss: 781.098\n",
      "[Epoch 11] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 12\n",
      "[Epoch 12, Batch 1] TRAIN loss: 818.374\n",
      "[Epoch 12, Batch 501] TRAIN loss: 769.700\n",
      "[Epoch 12, Batch 1001] TRAIN loss: 770.926\n",
      "[Epoch 12, Batch 1501] TRAIN loss: 777.075\n",
      "[Epoch 12] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 13\n",
      "[Epoch 13, Batch 1] TRAIN loss: 829.060\n",
      "[Epoch 13, Batch 501] TRAIN loss: 794.460\n",
      "[Epoch 13, Batch 1001] TRAIN loss: 789.128\n",
      "[Epoch 13, Batch 1501] TRAIN loss: 795.165\n",
      "[Epoch 13] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 14\n",
      "[Epoch 14, Batch 1] TRAIN loss: 753.841\n",
      "[Epoch 14, Batch 501] TRAIN loss: 778.232\n",
      "[Epoch 14, Batch 1001] TRAIN loss: 785.178\n",
      "[Epoch 14, Batch 1501] TRAIN loss: 781.208\n",
      "[Epoch 14] VALIDATION loss: 782.317\n",
      "==============================\n",
      "Starting epoch number 15\n",
      "[Epoch 15, Batch 1] TRAIN loss: 804.762\n",
      "[Epoch 15, Batch 501] TRAIN loss: 788.060\n",
      "[Epoch 15, Batch 1001] TRAIN loss: 798.187\n",
      "[Epoch 15, Batch 1501] TRAIN loss: 781.392\n",
      "[Epoch 15] VALIDATION loss: 782.317\n",
      "***************Autoencoder bottleneck size 16***************\n",
      "==============================\n",
      "Starting epoch number 1\n",
      "[Epoch 1, Batch 1] TRAIN loss: 782.642\n",
      "[Epoch 1, Batch 501] TRAIN loss: 790.729\n",
      "[Epoch 1, Batch 1001] TRAIN loss: 765.646\n",
      "[Epoch 1, Batch 1501] TRAIN loss: 781.779\n",
      "[Epoch 1] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 2\n",
      "[Epoch 2, Batch 1] TRAIN loss: 805.107\n",
      "[Epoch 2, Batch 501] TRAIN loss: 798.526\n",
      "[Epoch 2, Batch 1001] TRAIN loss: 789.876\n",
      "[Epoch 2, Batch 1501] TRAIN loss: 772.041\n",
      "[Epoch 2] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 3\n",
      "[Epoch 3, Batch 1] TRAIN loss: 799.420\n",
      "[Epoch 3, Batch 501] TRAIN loss: 804.831\n",
      "[Epoch 3, Batch 1001] TRAIN loss: 778.407\n",
      "[Epoch 3, Batch 1501] TRAIN loss: 791.358\n",
      "[Epoch 3] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 4\n",
      "[Epoch 4, Batch 1] TRAIN loss: 745.094\n",
      "[Epoch 4, Batch 501] TRAIN loss: 776.563\n",
      "[Epoch 4, Batch 1001] TRAIN loss: 782.519\n",
      "[Epoch 4, Batch 1501] TRAIN loss: 786.435\n",
      "[Epoch 4] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 5\n",
      "[Epoch 5, Batch 1] TRAIN loss: 794.384\n",
      "[Epoch 5, Batch 501] TRAIN loss: 778.298\n",
      "[Epoch 5, Batch 1001] TRAIN loss: 799.490\n",
      "[Epoch 5, Batch 1501] TRAIN loss: 786.380\n",
      "[Epoch 5] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 6\n",
      "[Epoch 6, Batch 1] TRAIN loss: 777.745\n",
      "[Epoch 6, Batch 501] TRAIN loss: 788.249\n",
      "[Epoch 6, Batch 1001] TRAIN loss: 775.720\n",
      "[Epoch 6, Batch 1501] TRAIN loss: 786.470\n",
      "[Epoch 6] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 7\n",
      "[Epoch 7, Batch 1] TRAIN loss: 726.066\n",
      "[Epoch 7, Batch 501] TRAIN loss: 791.221\n",
      "[Epoch 7, Batch 1001] TRAIN loss: 778.523\n",
      "[Epoch 7, Batch 1501] TRAIN loss: 799.752\n",
      "[Epoch 7] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 8\n",
      "[Epoch 8, Batch 1] TRAIN loss: 804.814\n",
      "[Epoch 8, Batch 501] TRAIN loss: 793.212\n",
      "[Epoch 8, Batch 1001] TRAIN loss: 778.146\n",
      "[Epoch 8, Batch 1501] TRAIN loss: 778.938\n",
      "[Epoch 8] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 9\n",
      "[Epoch 9, Batch 1] TRAIN loss: 836.668\n",
      "[Epoch 9, Batch 501] TRAIN loss: 761.790\n",
      "[Epoch 9, Batch 1001] TRAIN loss: 789.676\n",
      "[Epoch 9, Batch 1501] TRAIN loss: 786.652\n",
      "[Epoch 9] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 10\n",
      "[Epoch 10, Batch 1] TRAIN loss: 701.724\n",
      "[Epoch 10, Batch 501] TRAIN loss: 792.730\n",
      "[Epoch 10, Batch 1001] TRAIN loss: 800.284\n",
      "[Epoch 10, Batch 1501] TRAIN loss: 794.716\n",
      "[Epoch 10] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 11\n",
      "[Epoch 11, Batch 1] TRAIN loss: 824.800\n",
      "[Epoch 11, Batch 501] TRAIN loss: 779.173\n",
      "[Epoch 11, Batch 1001] TRAIN loss: 777.561\n",
      "[Epoch 11, Batch 1501] TRAIN loss: 790.331\n",
      "[Epoch 11] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 12\n",
      "[Epoch 12, Batch 1] TRAIN loss: 735.706\n",
      "[Epoch 12, Batch 501] TRAIN loss: 780.946\n",
      "[Epoch 12, Batch 1001] TRAIN loss: 793.750\n",
      "[Epoch 12, Batch 1501] TRAIN loss: 808.167\n",
      "[Epoch 12] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 13\n",
      "[Epoch 13, Batch 1] TRAIN loss: 830.454\n",
      "[Epoch 13, Batch 501] TRAIN loss: 785.306\n",
      "[Epoch 13, Batch 1001] TRAIN loss: 787.377\n",
      "[Epoch 13, Batch 1501] TRAIN loss: 774.792\n",
      "[Epoch 13] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 14\n",
      "[Epoch 14, Batch 1] TRAIN loss: 723.170\n",
      "[Epoch 14, Batch 501] TRAIN loss: 794.133\n",
      "[Epoch 14, Batch 1001] TRAIN loss: 782.916\n",
      "[Epoch 14, Batch 1501] TRAIN loss: 780.388\n",
      "[Epoch 14] VALIDATION loss: 782.164\n",
      "==============================\n",
      "Starting epoch number 15\n",
      "[Epoch 15, Batch 1] TRAIN loss: 729.656\n",
      "[Epoch 15, Batch 501] TRAIN loss: 779.924\n",
      "[Epoch 15, Batch 1001] TRAIN loss: 791.800\n",
      "[Epoch 15, Batch 1501] TRAIN loss: 781.127\n",
      "[Epoch 15] VALIDATION loss: 782.164\n",
      "***************Autoencoder bottleneck size 20***************\n",
      "==============================\n",
      "Starting epoch number 1\n",
      "[Epoch 1, Batch 1] TRAIN loss: 798.578\n",
      "[Epoch 1, Batch 501] TRAIN loss: 794.028\n",
      "[Epoch 1, Batch 1001] TRAIN loss: 793.718\n",
      "[Epoch 1, Batch 1501] TRAIN loss: 777.271\n",
      "[Epoch 1] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 2\n",
      "[Epoch 2, Batch 1] TRAIN loss: 747.871\n",
      "[Epoch 2, Batch 501] TRAIN loss: 798.833\n",
      "[Epoch 2, Batch 1001] TRAIN loss: 789.807\n",
      "[Epoch 2, Batch 1501] TRAIN loss: 788.006\n",
      "[Epoch 2] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 3\n",
      "[Epoch 3, Batch 1] TRAIN loss: 784.837\n",
      "[Epoch 3, Batch 501] TRAIN loss: 795.482\n",
      "[Epoch 3, Batch 1001] TRAIN loss: 789.786\n",
      "[Epoch 3, Batch 1501] TRAIN loss: 797.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 4\n",
      "[Epoch 4, Batch 1] TRAIN loss: 792.963\n",
      "[Epoch 4, Batch 501] TRAIN loss: 773.356\n",
      "[Epoch 4, Batch 1001] TRAIN loss: 791.957\n",
      "[Epoch 4, Batch 1501] TRAIN loss: 802.477\n",
      "[Epoch 4] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 5\n",
      "[Epoch 5, Batch 1] TRAIN loss: 736.825\n",
      "[Epoch 5, Batch 501] TRAIN loss: 774.838\n",
      "[Epoch 5, Batch 1001] TRAIN loss: 808.184\n",
      "[Epoch 5, Batch 1501] TRAIN loss: 789.004\n",
      "[Epoch 5] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 6\n",
      "[Epoch 6, Batch 1] TRAIN loss: 771.235\n",
      "[Epoch 6, Batch 501] TRAIN loss: 791.851\n",
      "[Epoch 6, Batch 1001] TRAIN loss: 797.599\n",
      "[Epoch 6, Batch 1501] TRAIN loss: 791.285\n",
      "[Epoch 6] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 7\n",
      "[Epoch 7, Batch 1] TRAIN loss: 801.068\n",
      "[Epoch 7, Batch 501] TRAIN loss: 786.874\n",
      "[Epoch 7, Batch 1001] TRAIN loss: 783.410\n",
      "[Epoch 7, Batch 1501] TRAIN loss: 788.397\n",
      "[Epoch 7] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 8\n",
      "[Epoch 8, Batch 1] TRAIN loss: 887.122\n",
      "[Epoch 8, Batch 501] TRAIN loss: 781.634\n",
      "[Epoch 8, Batch 1001] TRAIN loss: 768.719\n",
      "[Epoch 8, Batch 1501] TRAIN loss: 791.468\n",
      "[Epoch 8] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 9\n",
      "[Epoch 9, Batch 1] TRAIN loss: 806.129\n",
      "[Epoch 9, Batch 501] TRAIN loss: 796.166\n",
      "[Epoch 9, Batch 1001] TRAIN loss: 779.330\n",
      "[Epoch 9, Batch 1501] TRAIN loss: 780.803\n",
      "[Epoch 9] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 10\n",
      "[Epoch 10, Batch 1] TRAIN loss: 776.244\n",
      "[Epoch 10, Batch 501] TRAIN loss: 795.569\n",
      "[Epoch 10, Batch 1001] TRAIN loss: 802.132\n",
      "[Epoch 10, Batch 1501] TRAIN loss: 772.350\n",
      "[Epoch 10] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 11\n",
      "[Epoch 11, Batch 1] TRAIN loss: 723.542\n",
      "[Epoch 11, Batch 501] TRAIN loss: 780.084\n",
      "[Epoch 11, Batch 1001] TRAIN loss: 770.128\n",
      "[Epoch 11, Batch 1501] TRAIN loss: 792.377\n",
      "[Epoch 11] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 12\n",
      "[Epoch 12, Batch 1] TRAIN loss: 801.166\n",
      "[Epoch 12, Batch 501] TRAIN loss: 795.715\n",
      "[Epoch 12, Batch 1001] TRAIN loss: 773.666\n",
      "[Epoch 12, Batch 1501] TRAIN loss: 786.141\n",
      "[Epoch 12] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 13\n",
      "[Epoch 13, Batch 1] TRAIN loss: 696.239\n",
      "[Epoch 13, Batch 501] TRAIN loss: 790.233\n",
      "[Epoch 13, Batch 1001] TRAIN loss: 785.464\n",
      "[Epoch 13, Batch 1501] TRAIN loss: 785.007\n",
      "[Epoch 13] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 14\n",
      "[Epoch 14, Batch 1] TRAIN loss: 797.255\n",
      "[Epoch 14, Batch 501] TRAIN loss: 791.122\n",
      "[Epoch 14, Batch 1001] TRAIN loss: 815.330\n",
      "[Epoch 14, Batch 1501] TRAIN loss: 771.006\n",
      "[Epoch 14] VALIDATION loss: 780.986\n",
      "==============================\n",
      "Starting epoch number 15\n",
      "[Epoch 15, Batch 1] TRAIN loss: 753.106\n",
      "[Epoch 15, Batch 501] TRAIN loss: 782.092\n",
      "[Epoch 15, Batch 1001] TRAIN loss: 787.678\n",
      "[Epoch 15, Batch 1501] TRAIN loss: 788.290\n",
      "[Epoch 15] VALIDATION loss: 780.986\n"
     ]
    }
   ],
   "source": [
    "bottleneck_sizes = [4, 8, 12, 16, 20]\n",
    "autoencoders = {}\n",
    "for bottleneck_size in bottleneck_sizes:\n",
    "    print(\"*\"*15 + f\"Autoencoder bottleneck size {bottleneck_size}\" + \"*\"*15)\n",
    "    autoencoder = ConvolutionalAutoencoder(2, 'max-pooling', 'nearest', bottleneck_size=bottleneck_size).to(device)\n",
    "    autoencoders[f\"bottleneck_{bottleneck_size}\"] = (train(autoencoder, train_dataloader, val_dataloader, mse, adam, hparams[\"num_epochs\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bottleneck_4': ([796.8727264648315,\n",
       "   814.0396957022018,\n",
       "   779.0366403705914,\n",
       "   804.0274413400041,\n",
       "   786.5703865167659,\n",
       "   783.4169831921909,\n",
       "   820.8561410190322,\n",
       "   784.8186301498921,\n",
       "   825.9333383242777,\n",
       "   802.3549787746408,\n",
       "   799.1122753258854,\n",
       "   816.086036415483,\n",
       "   794.516998678388,\n",
       "   837.5767215623489,\n",
       "   811.5647133097755],\n",
       "  [796.5256217447917,\n",
       "   796.52562109375,\n",
       "   796.5256197916667,\n",
       "   796.5256243489583,\n",
       "   796.525619140625,\n",
       "   796.5256197916667,\n",
       "   796.5256223958334,\n",
       "   796.52562109375,\n",
       "   796.5256236979167,\n",
       "   796.5256217447917,\n",
       "   796.5256204427084,\n",
       "   796.5256204427084,\n",
       "   796.5256217447917,\n",
       "   796.5256204427084,\n",
       "   796.5256197916667]),\n",
       " 'bottleneck_8': ([790.5913308023371,\n",
       "   782.9490515439447,\n",
       "   793.4677693032294,\n",
       "   786.0014556219419,\n",
       "   784.118671680519,\n",
       "   774.7391167319926,\n",
       "   768.5633435638404,\n",
       "   790.0554776257175,\n",
       "   784.0662199501137,\n",
       "   773.0420164625102,\n",
       "   783.9684136718778,\n",
       "   785.1133982094357,\n",
       "   788.6871157551928,\n",
       "   798.4609362912762,\n",
       "   769.7919107100519],\n",
       "  [782.84744140625,\n",
       "   782.8474427083333,\n",
       "   782.84744140625,\n",
       "   782.8474420572917,\n",
       "   782.8474446614583,\n",
       "   782.847443359375,\n",
       "   782.8474407552084,\n",
       "   782.8474453125,\n",
       "   782.847443359375,\n",
       "   782.8474440104167,\n",
       "   782.84744140625,\n",
       "   782.8474440104167,\n",
       "   782.8474407552084,\n",
       "   782.8474407552084,\n",
       "   782.847439453125]),\n",
       " 'bottleneck_12': ([809.0971579975269,\n",
       "   771.7842770551064,\n",
       "   801.2023393712944,\n",
       "   794.2334828932856,\n",
       "   785.3328486232812,\n",
       "   771.9305441511374,\n",
       "   797.1879968456639,\n",
       "   780.855580855215,\n",
       "   814.8326988252944,\n",
       "   770.7832528752044,\n",
       "   796.9448560307733,\n",
       "   792.1789807305038,\n",
       "   784.4360753670915,\n",
       "   806.299641798054,\n",
       "   743.3795763889715],\n",
       "  [782.3173606770833,\n",
       "   782.3173619791667,\n",
       "   782.317359375,\n",
       "   782.3173626302083,\n",
       "   782.3173580729167,\n",
       "   782.317359375,\n",
       "   782.3173626302083,\n",
       "   782.3173580729167,\n",
       "   782.317359375,\n",
       "   782.3173600260417,\n",
       "   782.3173619791667,\n",
       "   782.317361328125,\n",
       "   782.317361328125,\n",
       "   782.317359375,\n",
       "   782.3173587239584]),\n",
       " 'bottleneck_16': ([797.1223142344351,\n",
       "   759.4032959031816,\n",
       "   800.8529337940872,\n",
       "   778.3730279446523,\n",
       "   781.6317993146967,\n",
       "   776.571142934775,\n",
       "   793.4553762475966,\n",
       "   782.9441263694747,\n",
       "   787.8196223953065,\n",
       "   801.9385213069015,\n",
       "   797.9734687365728,\n",
       "   812.0439884031376,\n",
       "   786.7252958527782,\n",
       "   775.6671478241677,\n",
       "   768.5685628139582],\n",
       "  [782.1641438802084,\n",
       "   782.1641451822917,\n",
       "   782.1641451822917,\n",
       "   782.1641419270833,\n",
       "   782.1641432291667,\n",
       "   782.1641451822917,\n",
       "   782.1641432291667,\n",
       "   782.164146484375,\n",
       "   782.1641451822917,\n",
       "   782.1641438802084,\n",
       "   782.1641432291667,\n",
       "   782.1641438802084,\n",
       "   782.16414453125,\n",
       "   782.1641432291667,\n",
       "   782.164142578125]),\n",
       " 'bottleneck_20': ([782.3964623205143,\n",
       "   810.9690217225726,\n",
       "   794.2194848460182,\n",
       "   791.4064984073952,\n",
       "   790.4859555494986,\n",
       "   798.3080400664011,\n",
       "   771.7157540508821,\n",
       "   780.9947732852787,\n",
       "   771.8319028265989,\n",
       "   773.9568230331425,\n",
       "   787.896285712058,\n",
       "   793.3060606686175,\n",
       "   779.2956223356903,\n",
       "   798.5383049127886,\n",
       "   772.6875989216364],\n",
       "  [780.9859453125,\n",
       "   780.9859459635417,\n",
       "   780.9859466145833,\n",
       "   780.9859479166666,\n",
       "   780.9859505208333,\n",
       "   780.9859459635417,\n",
       "   780.9859453125,\n",
       "   780.9859498697916,\n",
       "   780.9859498697916,\n",
       "   780.9859479166666,\n",
       "   780.9859440104167,\n",
       "   780.9859427083334,\n",
       "   780.9859453125,\n",
       "   780.9859453125,\n",
       "   780.985947265625])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoders"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "team08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
