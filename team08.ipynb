{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaWaYXG1fIRm",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GA4Aiz2Giu5e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'batch_size':32,\n",
    "    'num_epochs':10,\n",
    "    'test_batch_size':32,\n",
    "    'num_workers': 4,\n",
    "    'train_percentage': 0.95,\n",
    "    'hidden_size':128,\n",
    "    'num_classes':10,\n",
    "    'num_inputs':784,\n",
    "    'learning_rate':1e-3,\n",
    "    'log_interval':500,\n",
    "}\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyK3i7pS9gSH"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Vf3xEV29k0y"
   },
   "source": [
    "## Module definition\n",
    "\n",
    "Our autoencoder will be defined by the `ConvolutionalAutoencoder` class which uses a `ConvolutionalEncoder` object to encode followed a `ConvolutionalDecoder` object to decode. The `ConvolutionalEncoder` and `ConvolutionalDecoder` classes make use of `n_blocks` `ConvolutionalBlock`s or `DeconvolutionalBlock`s which are composed of `layer_per_block` convolution layers with the same number of filters.\n",
    "\n",
    "The dimensionality is reduced by applying 2-factor spatial downsampling at each block. The number of filters is doubled for each subsequent block. The decoder makes the exact oposite process.\n",
    "\n",
    "The final layer uses a tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6ZgDF0kfIRy",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_blocks, downsampling_method, upsampling_method,\n",
    "                 layers_per_block=2):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.downsampling_method = downsampling_method\n",
    "        self.upsampling_method = upsampling_method\n",
    "\n",
    "        self.encoder = ConvolutionalEncoder(n_blocks, downsampling_method,\n",
    "                                            layers_per_block=layers_per_block)\n",
    "        self.decoder = ConvolutionalDecoder(n_blocks, upsampling_method,\n",
    "                                            self.encoder.output_channels,\n",
    "                                            layers_per_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        reconstruction = self.decoder(code)\n",
    "        return reconstruction\n",
    "\n",
    "\n",
    "class ConvolutionalEncoder(torch.nn.Module):\n",
    "\n",
    "    DOWNSAMPLING_METHODS = [\"max-pooling\", \"avg-pooling\", \"stride-2\"]\n",
    "\n",
    "    def __init__(self, n_blocks, downsampling_method, init_filters=16,\n",
    "                 layers_per_block=2, kernel_size=5, input_channels=1):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        assert downsampling_method in self.DOWNSAMPLING_METHODS\n",
    "        self.downsampling_method = downsampling_method\n",
    "        self.layers_per_block = layers_per_block\n",
    "        self.init_filters = init_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # First layer so we have <init_filters> channels.\n",
    "        n_filters = init_filters\n",
    "        layers.append(\n",
    "            ConvolutionalBlock(input_channels, n_filters, kernel_size, 1))\n",
    "\n",
    "        # Encoding blocks.\n",
    "        input_channels = n_filters\n",
    "        for _ in range(n_blocks):\n",
    "            if downsampling_method == \"max-pooling\":\n",
    "                # Convolutional block + max pooling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, n_filters, kernel_size, layers_per_block),\n",
    "                    torch.nn.MaxPool2d(2)\n",
    "                )\n",
    "            elif downsampling_method == \"avg-pooling\":\n",
    "                # Convolutional block + average pooling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, n_filters, kernel_size, layers_per_block),\n",
    "                    torch.nn.AvgPool2d(2)\n",
    "                )\n",
    "            else:\n",
    "                # Stride-2 convolution.\n",
    "                conv_block = ConvolutionalBlock(input_channels, n_filters,\n",
    "                                                kernel_size,\n",
    "                                                layers_per_block,\n",
    "                                                last_stride=2)\n",
    "            layers.append(conv_block)\n",
    "            # Double the number of filters.\n",
    "            input_channels = n_filters\n",
    "            n_filters = 2 * n_filters\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*layers)\n",
    "        self.output_channels = input_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class ConvolutionalDecoder(torch.nn.Module):\n",
    "\n",
    "    UPSAMPLING_METHODS = [\"transposed\", \"bilinear\", \"bicubic\", \"nearest\"]\n",
    "\n",
    "    def __init__(self, n_blocks, upsampling_method, input_channels,\n",
    "                 layers_per_block=2, kernel_size=5, output_channels=1):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        assert upsampling_method in self.UPSAMPLING_METHODS\n",
    "        self.upsampling_method = upsampling_method\n",
    "        self.layers_per_block = layers_per_block\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Decoding blocks.\n",
    "        n_filters = input_channels\n",
    "        for _ in range(n_blocks):\n",
    "            if upsampling_method == \"transposed\":\n",
    "                # Deconvolutional block\n",
    "                conv_block = DeconvolutionalBlock(input_channels, n_filters,\n",
    "                                                  kernel_size, layers_per_block,\n",
    "                                                  stride=2)\n",
    "            else:\n",
    "                # Upsampling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, n_filters, kernel_size,\n",
    "                                       layers_per_block),\n",
    "                    torch.nn.Upsample(scale_factor=2, mode=upsampling_method)\n",
    "                )\n",
    "            layers.append(conv_block)\n",
    "            # Half the number of filters.\n",
    "            input_channels = n_filters\n",
    "            n_filters = n_filters // 2\n",
    "\n",
    "        # Last layer so we have <output_channels> channel.\n",
    "        layers.append(torch.nn.Conv2d(input_channels, output_channels, kernel_size,\n",
    "                                      padding=kernel_size // 2))\n",
    "        layers.append(torch.nn.Tanh())\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class ConvolutionalBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Applies n_layers convolutional layers with the same number of\n",
    "    filters (n_filters) and filter sizes (kerne_size) with ReLU activations\n",
    "    keeping the same spacial size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, n_filters, kernel_size, n_layers,\n",
    "                 last_stride=1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        padding = kernel_size // 2  # To keep the same size.\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:  # First layer with correct input channels.\n",
    "                layers.append(torch.nn.Conv2d(input_channels, n_filters,\n",
    "                                              kernel_size, padding=padding))\n",
    "            elif 0 < i < n_layers:  # Intermediate layers.\n",
    "                layers.append(torch.nn.Conv2d(n_filters, n_filters,\n",
    "                                              kernel_size, padding=padding))\n",
    "            else:  # Last layer with stride.\n",
    "                layers.append(torch.nn.Conv2d(n_filters, n_filters,\n",
    "                                              kernel_size, last_stride, padding))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # To sequentially apply the layers.\n",
    "        self.block = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class DeconvolutionalBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Applies a transposed convolution followed by n_layers-1 convolutional\n",
    "    layers with the same number of filters and filter sizes with ReLU\n",
    "    activations keeping the same spacial size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, n_filters, kernel_size, n_layers, stride):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Transposed convolution layer.\n",
    "        layers.append(torch.nn.ConvTranspose2d(input_channels, n_filters,\n",
    "                                               kernel_size, stride, padding))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(torch.nn.Conv2d(n_filters, n_filters, kernel_size,\n",
    "                                          padding=padding))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # To sequentially apply the layers.\n",
    "        self.block = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_ZD0rgWBhJa"
   },
   "source": [
    "To do a quick test, we will pass a random image and check if the output is of the same size as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIO0AZWCfIR2",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "image = torch.randn((1, 1, 128, 128))\n",
    "autoencoder = ConvolutionalAutoencoder(2, 'max-pooling', 'nearest').to(device)\n",
    "output = autoencoder(image.to(device))\n",
    "assert output.shape == (1, 1, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z25DsijeCNvi"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We split the training set and normalize the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObQndlPufIR9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transfoms.Compose([\n",
    "     torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.1307,), (0.3081,))  # Mean and std from internet...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11325,
     "status": "ok",
     "timestamp": 1575228638843,
     "user": {
      "displayName": "Sergio Garcia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDTm-mE3fT-h7wpEZpGtZu9jSK24Hqhb2nqpKiyysE=s64",
      "userId": "16139820604268433723"
     },
     "user_tz": -60
    },
    "id": "1VIGvzG4fISD",
    "outputId": "3bc1ae58-6e5f-4a3f-8e2e-f678fba722af",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mnist = torch.dataset.MNIST('mnist_dataset', train=True, transform=transform, download=True)\n",
    "dataset_len = len(mnist)\n",
    "train_size = hparams[\"train_percentage\"] * dataset_len\n",
    "val_size = (1 - hparams[\"train_percentage\"]) * dataset_len\n",
    "\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5yH0O1kfISY",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(mnist_train,\n",
    "                                               batch_size=hparams[\"train_batch_size\"],\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=hparams[\"num_workers\"])\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(mnist_val,\n",
    "                                               batch_size=hparams[\"test_batch_size\"],\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=hparams[\"num_workers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section defines the loss and the optimizer. In this case we will use Mean Squared Error loss as we are making the reconstruction of the image and using Adam as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNNQ25Z-fISe",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss()\n",
    "adam = torch.optim.Adam(autoencoder.parameters(), lr=hparams[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEwsot_KfISi",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epoch):\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    avg_loss = None\n",
    "    exp_weight = 0.1 #for exponential weighted avg of the loss\n",
    "    for batch_index, (data, _) in enumerate(dataloader):\n",
    "        images = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = model(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "        loss.backward()\n",
    "        \n",
    "        if avg_loss:\n",
    "            avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\n",
    "        else:\n",
    "            avg_loss = loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % hparams[\"log_interval\"] == 0:\n",
    "            print('[Epoch %d, Batch %d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss.item()))\n",
    "        train_loss.append(avg_loss)\n",
    "    return train_loss\n",
    "\n",
    "def val(model, dataloader, criterion, optimizer, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (data, _) in enumerate(dataloader):\n",
    "            images = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(images)\n",
    "            test_loss += criterion(reconstructed, images, reduction='sum').item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91626,
     "status": "ok",
     "timestamp": 1575228794730,
     "user": {
      "displayName": "Sergio Garcia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDTm-mE3fT-h7wpEZpGtZu9jSK24Hqhb2nqpKiyysE=s64",
      "userId": "16139820604268433723"
     },
     "user_tz": -60
    },
    "id": "CXQlTiq2fISm",
    "outputId": "4c8762a1-882e-4092-ae1d-d01a899bc9c0",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1] loss: 0.924\n",
      "[1, 101] loss: 0.413\n",
      "[1, 201] loss: 0.377\n",
      "[1, 301] loss: 0.382\n",
      "[1, 401] loss: 0.348\n",
      "[1, 501] loss: 0.365\n",
      "[1, 601] loss: 0.343\n",
      "[1, 701] loss: 0.337\n",
      "[1, 801] loss: 0.359\n",
      "[1, 901] loss: 0.395\n",
      "[1, 1001] loss: 0.356\n",
      "[1, 1101] loss: 0.352\n",
      "[1, 1201] loss: 0.355\n",
      "[1, 1301] loss: 0.366\n",
      "[1, 1401] loss: 0.338\n",
      "[1, 1501] loss: 0.306\n",
      "[1, 1601] loss: 0.299\n",
      "[1, 1701] loss: 0.337\n",
      "[2, 1] loss: 0.335\n",
      "[2, 101] loss: 0.341\n",
      "[2, 201] loss: 0.358\n",
      "[2, 301] loss: 0.309\n",
      "[2, 401] loss: 0.357\n",
      "[2, 501] loss: 0.288\n",
      "[2, 601] loss: 0.350\n",
      "[2, 701] loss: 0.329\n",
      "[2, 801] loss: 0.312\n",
      "[2, 901] loss: 0.335\n",
      "[2, 1001] loss: 0.344\n",
      "[2, 1101] loss: 0.351\n",
      "[2, 1201] loss: 0.330\n",
      "[2, 1301] loss: 0.327\n",
      "[2, 1401] loss: 0.346\n",
      "[2, 1501] loss: 0.298\n",
      "[2, 1601] loss: 0.307\n",
      "[2, 1701] loss: 0.312\n",
      "[3, 1] loss: 0.325\n",
      "[3, 101] loss: 0.309\n",
      "[3, 201] loss: 0.344\n",
      "[3, 301] loss: 0.354\n",
      "[3, 401] loss: 0.301\n",
      "[3, 501] loss: 0.275\n",
      "[3, 601] loss: 0.294\n",
      "[3, 701] loss: 0.343\n",
      "[3, 801] loss: 0.302\n",
      "[3, 901] loss: 0.303\n",
      "[3, 1001] loss: 0.321\n",
      "[3, 1101] loss: 0.331\n",
      "[3, 1201] loss: 0.357\n",
      "[3, 1301] loss: 0.310\n",
      "[3, 1401] loss: 0.337\n",
      "[3, 1501] loss: 0.350\n",
      "[3, 1601] loss: 0.332\n",
      "[3, 1701] loss: 0.360\n",
      "[4, 1] loss: 0.338\n",
      "[4, 101] loss: 0.317\n",
      "[4, 201] loss: 0.339\n",
      "[4, 301] loss: 0.328\n",
      "[4, 401] loss: 0.309\n",
      "[4, 501] loss: 0.323\n",
      "[4, 601] loss: 0.350\n",
      "[4, 701] loss: 0.318\n",
      "[4, 801] loss: 0.321\n",
      "[4, 901] loss: 0.329\n",
      "[4, 1001] loss: 0.306\n",
      "[4, 1101] loss: 0.346\n",
      "[4, 1201] loss: 0.318\n",
      "[4, 1301] loss: 0.315\n",
      "[4, 1401] loss: 0.331\n",
      "[4, 1501] loss: 0.335\n",
      "[4, 1601] loss: 0.340\n",
      "[4, 1701] loss: 0.292\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fc71945de0a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-1bb443790a20>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\worldsensing\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\worldsensing\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(hparams[\"num_epochs\"]):\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Starting epoch number {epoch}\")\n",
    "    train(autoencoder, train_dataloader, mse, adam, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OKrrQKRfISs",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mnist_test = torchvision.datasets.MNIST('mnist_dataset', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQh4_KZ8h5Aj",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "reconstructed = autoencoder(mnist_test[0][0].to(device).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmq0GFneiCDo",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1131,
     "status": "ok",
     "timestamp": 1575228917871,
     "user": {
      "displayName": "Sergio Garcia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDTm-mE3fT-h7wpEZpGtZu9jSK24Hqhb2nqpKiyysE=s64",
      "userId": "16139820604268433723"
     },
     "user_tz": -60
    },
    "id": "dZnZDQXqiGgv",
    "outputId": "6ad1ca25-3c71-4b48-b64f-e49ded890343",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(mnist_test[0][0][0], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(reconstructed[0][0].detach().cpu().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyEnlw7JiJ-X",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "team08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}