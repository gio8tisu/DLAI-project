{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaWaYXG1fIRm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GA4Aiz2Giu5e"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MjS3A1UNFJF"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS_AUTOENCODER = 10\n",
    "EPOCHS_PRETRAINING = 20\n",
    "EPOCHS_FINETUNING = 20\n",
    "EPOCHS_VAE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyK3i7pS9gSH"
   },
   "source": [
    "# Exercise 1: Convolutional Autoencoder\n",
    "\n",
    "Steps:\n",
    "1. Load MNIST train and test sets. Split the original training data into 95% training and 5% validation data.\n",
    "2. Implement a convolutional autoencoder (with separate Encoder and Decoder modules).\n",
    "3. Train the convolutional autoencoder, with different bottleneck sizes. Plot the train and validation loss curves of all autoencoders in the same figure.\n",
    "4. Compute the avg. image reconstruction error (MSE) of the trained models on the MNIST validation and test sets. Show the results in a table, including #params of each model.\n",
    "5. Select one of the autoencoders and feed it 5 random MNIST images from the test set. Show them along with their reconstructions.\n",
    "Generate 5 new images by injecting random values as input to the decoder. Show them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Vf3xEV29k0y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Module definition\n",
    "\n",
    "Our autoencoder will be defined by the `ConvolutionalAutoencoder` class which uses a `ConvolutionalEncoder` object to encode followed a `ConvolutionalDecoder` object to decode. The `ConvolutionalEncoder` and `ConvolutionalDecoder` classes make use of `n_blocks` `ConvolutionalBlock`s or `DeconvolutionalBlock`s which are composed of `layer_per_block` convolution layers with the same number of filters.\n",
    "\n",
    "The dimensionality is reduced by applying 2-factor spatial downsampling at each block. The number of filters is doubled for each subsequent block. The decoder makes the exact oposite process.\n",
    "\n",
    "The final layer uses a tanh activation.\n",
    "\n",
    "Very similar to the `ConvolutionalAutoencoder`, there is the `ConvolutionalAutoencoderReducedLatentDim` class which includes 2 fully-connected layers between the encoder and decoder. The first one projects from the encoder's feature maps to a vector of `latent_dimensionality` dimensions, the second one does the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EknaWIjxJUZ1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %load models.py\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "\n",
    "class ConvolutionalAutoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_blocks, downsampling_method, upsampling_method,\n",
    "                 layers_per_block=2):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.downsampling_method = downsampling_method\n",
    "        self.upsampling_method = upsampling_method\n",
    "\n",
    "        self.encoder = ConvolutionalEncoder(n_blocks, downsampling_method,\n",
    "                                            layers_per_block=layers_per_block)\n",
    "        self.decoder = ConvolutionalDecoder(n_blocks, upsampling_method,\n",
    "                                            self.encoder.output_channels,\n",
    "                                            layers_per_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        reconstruction = self.decoder(code)\n",
    "        return reconstruction\n",
    "\n",
    "\n",
    "class ConvolutionalAutoencoderReducedLatentDim(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, n_blocks, downsampling_method, upsampling_method,\n",
    "                 layers_per_block=2, latent_dimensionality=50):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.downsampling_method = downsampling_method\n",
    "        self.upsampling_method = upsampling_method\n",
    "        self.latent_dimensionality = latent_dimensionality\n",
    "\n",
    "        # Encoder: Convolutional blocks + Linear\n",
    "        self.convolutional_encoder = ConvolutionalEncoder(\n",
    "            n_blocks, downsampling_method, layers_per_block=layers_per_block)\n",
    "        self.encoder_output_shape = (self.convolutional_encoder.init_filters * 2 ** (n_blocks - 1),\n",
    "                                     input_shape[0] // 2 ** n_blocks,\n",
    "                                     input_shape[1] // 2 ** n_blocks)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            self.convolutional_encoder,\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(self.encoder_output_shape[0] * self.encoder_output_shape[1] * self.encoder_output_shape[2],\n",
    "                            latent_dimensionality),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder: Linear + Convolutional blocks\n",
    "        self.linear_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dimensionality,\n",
    "                            self.encoder_output_shape[0] * self.encoder_output_shape[1] * self.encoder_output_shape[2]),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.convolutional_decoder = ConvolutionalDecoder(\n",
    "            n_blocks, upsampling_method,\n",
    "            self.convolutional_encoder.output_channels, layers_per_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        reconstruction = self.convolutional_decoder(\n",
    "            self.linear_decoder(code).view((-1,) + self.encoder_output_shape))\n",
    "        return reconstruction\n",
    "\n",
    "\n",
    "class ConvolutionalEncoder(torch.nn.Module):\n",
    "    DOWNSAMPLING_METHODS = [\"max-pooling\", \"avg-pooling\", \"stride-2\"]\n",
    "\n",
    "    def __init__(self, n_blocks, downsampling_method, init_filters=16,\n",
    "                 layers_per_block=2, kernel_size=5, input_channels=1):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        assert downsampling_method in self.DOWNSAMPLING_METHODS\n",
    "        self.downsampling_method = downsampling_method\n",
    "        self.layers_per_block = layers_per_block\n",
    "        self.init_filters = init_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # First layer so we have <input_channels> channels.\n",
    "        n_filters = init_filters\n",
    "        layers.append(\n",
    "            ConvolutionalBlock(input_channels, n_filters, kernel_size, 1))\n",
    "\n",
    "        # Encoding blocks.\n",
    "        input_channels = n_filters\n",
    "        for _ in range(n_blocks):\n",
    "            if downsampling_method == \"max-pooling\":\n",
    "                # Convolutional block + max pooling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, n_filters, kernel_size,\n",
    "                                       layers_per_block),\n",
    "                    torch.nn.MaxPool2d(2)\n",
    "                )\n",
    "            elif downsampling_method == \"avg-pooling\":\n",
    "                # Convolutional block + average pooling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, n_filters, kernel_size,\n",
    "                                       layers_per_block),\n",
    "                    torch.nn.AvgPool2d(2)\n",
    "                )\n",
    "            else:\n",
    "                # Stride-2 convolution.\n",
    "                conv_block = ConvolutionalBlock(input_channels, n_filters,\n",
    "                                                kernel_size,\n",
    "                                                layers_per_block,\n",
    "                                                last_stride=2)\n",
    "            layers.append(conv_block)\n",
    "            # Double the number of filters.\n",
    "            input_channels = n_filters\n",
    "            n_filters = 2 * n_filters\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*layers)\n",
    "        self.output_channels = input_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class ConvolutionalDecoder(torch.nn.Module):\n",
    "    UPSAMPLING_METHODS = [\"transposed\", \"bilinear\", \"bicubic\", \"nearest\"]\n",
    "\n",
    "    def __init__(self, n_blocks, upsampling_method, input_channels,\n",
    "                 layers_per_block=2, kernel_size=5, output_channels=1,\n",
    "                 activation=\"tanh\"):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        assert upsampling_method in self.UPSAMPLING_METHODS\n",
    "        self.upsampling_method = upsampling_method\n",
    "        self.layers_per_block = layers_per_block\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_channels = output_channels\n",
    "        self.activation = activation\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Decoding blocks.\n",
    "        n_filters = input_channels\n",
    "        for _ in range(n_blocks):\n",
    "            if upsampling_method == \"transposed\":\n",
    "                # Deconvolutional block\n",
    "                conv_block = DeconvolutionalBlock(input_channels, n_filters,\n",
    "                                                  kernel_size, layers_per_block,\n",
    "                                                  stride=2)\n",
    "            else:\n",
    "                # Upsampling.\n",
    "                conv_block = torch.nn.Sequential(\n",
    "                    ConvolutionalBlock(input_channels, n_filters, kernel_size,\n",
    "                                       layers_per_block),\n",
    "                    torch.nn.Upsample(scale_factor=2, mode=upsampling_method)\n",
    "                )\n",
    "            layers.append(conv_block)\n",
    "            # Half the number of filters.\n",
    "            input_channels = n_filters\n",
    "            n_filters = n_filters // 2\n",
    "\n",
    "        # Last layer so we have <output_channels> channel.\n",
    "        layers.append(torch.nn.Conv2d(input_channels, output_channels, kernel_size,\n",
    "                                      padding=kernel_size // 2))\n",
    "        if activation == \"tanh\":\n",
    "            layers.append(torch.nn.Tanh())\n",
    "        elif activation == \"sigmoid\":\n",
    "            layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class ConvolutionalBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Applies n_layers convolutional layers with the same number of\n",
    "    filters and filter sizes with ReLU activations\n",
    "    keeping the same spacial size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, n_filters, kernel_size, n_layers,\n",
    "                 last_stride=1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        padding = kernel_size // 2  # To keep the same size.\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:  # First layer with correct input channels.\n",
    "                layers.append(torch.nn.Conv2d(input_channels, n_filters,\n",
    "                                              kernel_size, padding=padding))\n",
    "            elif 0 < i < n_layers:  # Intermediate layers.\n",
    "                layers.append(torch.nn.Conv2d(n_filters, n_filters,\n",
    "                                              kernel_size, padding=padding))\n",
    "            else:  # Last layer with stride.\n",
    "                layers.append(torch.nn.Conv2d(n_filters, n_filters,\n",
    "                                              kernel_size, last_stride, padding))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # To sequentially apply the layers.\n",
    "        self.block = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class DeconvolutionalBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Applies a transposed convolution followed by n_layers-1 convolutional\n",
    "    layers with the same number of filters and filter sizes with ReLU\n",
    "    activations keeping the same spacial size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, n_filters, kernel_size, n_layers, stride):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Transposed convolution layer.\n",
    "        layers.append(torch.nn.ConvTranspose2d(input_channels, n_filters,\n",
    "                                               kernel_size, stride, padding, 1))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(torch.nn.Conv2d(n_filters, n_filters, kernel_size,\n",
    "                                          padding=padding))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # To sequentially apply the layers.\n",
    "        self.block = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_ZD0rgWBhJa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To do a quick test, we will pass a random image and check if the output is of the same size as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIO0AZWCfIR2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image = torch.randn((10, 1, 128, 128))\n",
    "autoencoder = ConvolutionalAutoencoderReducedLatentDim(input_shape=(128, 128),\n",
    "                                                       n_blocks=2,\n",
    "                                                       downsampling_method='max-pooling',\n",
    "                                                       upsampling_method='nearest',\n",
    "                                                       layers_per_block=2,\n",
    "                                                       latent_dimensionality=50)\n",
    "output = autoencoder(image)\n",
    "assert output.shape == (10, 1, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z25DsijeCNvi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We split the training set and normalize the input images with mean and variance of the training set found on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObQndlPufIR9"
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "     torchvision.transforms.ToTensor(),\n",
    "     # torchvision.transforms.Normalize((0.1307,), (0.3081,))  # Mean and std from internet...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VIGvzG4fISD"
   },
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.MNIST('mnist_dataset', train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5cYwBL2fISH"
   },
   "outputs": [],
   "source": [
    "dataset_len = len(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QPrTXSnZfISN",
    "outputId": "9ac1916a-ed75-486d-d96a-b91f6efe7728"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_len * 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1BwFJY_fISU"
   },
   "outputs": [],
   "source": [
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist, [57000, dataset_len - 57000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5yH0O1kfISY"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(mnist_train,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ybj6UC1qJUaN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(mnist_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZpKJsSBcc4b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To create our autoencoder we need to know the size of our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fqt6_aCtJUaR",
    "outputId": "5c81f17e-00b3-42fe-f83f-fb5c431227c6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = mnist_train[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCP_Zd2rDqlW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wEm9Aahock8U",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will first train an autoencoder with a bottleneck of size 50, **2 blocks of 2 convolutional layers** on each block (each in the encoder and decoder), using **max-pooling** for downsampling and **nearest-neighbour** for upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIYstBcDJUaV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder = ConvolutionalAutoencoderReducedLatentDim(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=50\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xt3wsOghdxHh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use the mean square error as our loss function and Adam as our optimization algorithm with a learning rate of 0.001.  \n",
    "We use 'sum' as reduction to compute the image reconstruction error instead of the pixel reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNNQ25Z-fISe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss(reduction='sum')\n",
    "adam = torch.optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RX893gG4eEf2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define our `train` function that will be called at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEwsot_KfISi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epoch, loss_history, log_interval=100):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        images = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = model(images)\n",
    "        # Divide by batch size to compute mean image reconstruction error.\n",
    "        loss = criterion(reconstructed, images) / images.shape[0]\n",
    "        loss.backward()   # Backprop.\n",
    "        optimizer.step()  # Parameter updates.\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        if i % log_interval == 0:\n",
    "            print('[%d, %d] loss: %.3f' % (epoch + 1, i, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zo8uYtLgeC6o",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Very similar to the `train` function, we define our `test` function to validate. In this case we don't backpropagate and update our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GoV4rvPyJUac",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, dataloader, criterion, epoch, loss_history):\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    for batch in dataloader:\n",
    "        images = batch[0].to(device)\n",
    "        reconstructed = model(images)\n",
    "        loss += criterion(reconstructed, images).item()\n",
    "        total += images.shape[0]\n",
    "\n",
    "    mean_loss = loss / total\n",
    "    loss_history.append(mean_loss)  \n",
    "    print('[%d, test] loss: %.3f' % (epoch + 1, mean_loss))\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vfBCQHqepWO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is our training loop. Firstly, we call the `test` function so our validation loss curve includes the initial/untrained \"behaviour\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CXQlTiq2fISm",
    "outputId": "9129f914-d77e-4365-825f-bc3ba9d16498",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, test] loss: 100.729\n",
      "[1, 0] loss: 95.390\n",
      "[1, 100] loss: 52.128\n",
      "[1, 200] loss: 51.179\n",
      "[1, 300] loss: 53.715\n",
      "[1, 400] loss: 54.406\n",
      "[2, test] loss: 53.320\n",
      "[2, 0] loss: 50.304\n",
      "[2, 100] loss: 38.084\n",
      "[2, 200] loss: 24.764\n",
      "[2, 300] loss: 19.892\n",
      "[2, 400] loss: 16.899\n",
      "[3, test] loss: 16.242\n",
      "[3, 0] loss: 16.269\n",
      "[3, 100] loss: 13.912\n",
      "[3, 200] loss: 14.396\n",
      "[3, 300] loss: 12.585\n",
      "[3, 400] loss: 11.925\n",
      "[4, test] loss: 12.332\n",
      "[4, 0] loss: 11.620\n",
      "[4, 100] loss: 12.256\n",
      "[4, 200] loss: 10.659\n",
      "[4, 300] loss: 11.703\n",
      "[4, 400] loss: 10.163\n",
      "[5, test] loss: 11.068\n",
      "[5, 0] loss: 11.128\n",
      "[5, 100] loss: 10.741\n",
      "[5, 200] loss: 10.541\n",
      "[5, 300] loss: 11.209\n",
      "[5, 400] loss: 10.598\n",
      "[6, test] loss: 10.292\n",
      "[6, 0] loss: 10.222\n",
      "[6, 100] loss: 10.726\n",
      "[6, 200] loss: 10.522\n",
      "[6, 300] loss: 9.127\n",
      "[6, 400] loss: 9.909\n",
      "[7, test] loss: 10.109\n",
      "[7, 0] loss: 10.031\n",
      "[7, 100] loss: 9.251\n",
      "[7, 200] loss: 9.235\n",
      "[7, 300] loss: 9.464\n",
      "[7, 400] loss: 8.912\n",
      "[8, test] loss: 9.637\n",
      "[8, 0] loss: 9.913\n",
      "[8, 100] loss: 8.807\n",
      "[8, 200] loss: 9.362\n",
      "[8, 300] loss: 8.566\n",
      "[8, 400] loss: 8.157\n",
      "[9, test] loss: 8.800\n",
      "[9, 0] loss: 7.632\n",
      "[9, 100] loss: 8.536\n",
      "[9, 200] loss: 9.686\n",
      "[9, 300] loss: 8.848\n",
      "[9, 400] loss: 8.301\n",
      "[10, test] loss: 8.675\n",
      "[10, 0] loss: 8.214\n",
      "[10, 100] loss: 8.432\n",
      "[10, 200] loss: 7.599\n",
      "[10, 300] loss: 8.498\n",
      "[10, 400] loss: 8.660\n",
      "[10, test] loss: 8.326\n"
     ]
    }
   ],
   "source": [
    "training_loss = []    #\n",
    "validation_loss = []  # Here we save the training process.\n",
    "\n",
    "for epoch in range(EPOCHS_AUTOENCODER):\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = test(autoencoder, val_dataloader, mse, epoch, validation_loss)\n",
    "    autoencoder.train()\n",
    "    train(autoencoder, train_dataloader, mse, adam, epoch, training_loss)\n",
    "    # if early_stopping(validation_loss):\n",
    "    #     break\n",
    "\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    test(autoencoder, val_dataloader, mse, epoch, validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "4I6vt-OjST-I",
    "outputId": "e3892f27-a2d1-4baa-8623-4f1ffcd7df09",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Autoencoder reconstruction loss curves')\n",
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(np.linspace(0, len(training_loss), len(validation_loss)), validation_loss, '-x', label='validation loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXnbmjhLfaTR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that the loss quickly reduces and then it doesn't improve much. The validation curve is similar to the training one, so we can say it doesn't overfit the training data.\n",
    "\n",
    "In log-scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "BxrbIH0Qgcpn",
    "outputId": "f471e937-8d34-4734-b65a-de90a8b3f134",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Autoencoder reconstruction loss curves in log-scale')\n",
    "plt.yscale('log')\n",
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(np.linspace(0, len(training_loss), len(validation_loss)), validation_loss, '-x', label='validation loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3FTDWqo4NcF0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we'll try to reconstruct a image from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OKrrQKRfISs"
   },
   "outputs": [],
   "source": [
    "mnist_test = torchvision.datasets.MNIST('mnist_dataset', train=False, transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(mnist_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfXE9nSUQBV1"
   },
   "outputs": [],
   "source": [
    "def show_image(image, autoencoder, title=None):\n",
    "    reconstructed = autoencoder(image.to(device).unsqueeze(0))\n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    plt.subplot(121).set_title('Original image')\n",
    "    plt.imshow(image.reshape((28, 28)), cmap='gray')\n",
    "    plt.subplot(122).set_title('Reconstructed image')\n",
    "    plt.imshow(reconstructed.reshape((28, 28)).detach().cpu().numpy(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "7YUCupgTgYt3",
    "outputId": "1cb7abca-b402-4b9d-ac00-f70298ea59e7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(mnist_test[6][0], autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lxb8rpY3goVF"
   },
   "source": [
    "We can see some information is loss, but the reconstruction is pretty accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQfkKa71EQ9s",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Varying bottleneck size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34WQ01WqLcmz"
   },
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3ctr05S_Edn6",
    "outputId": "1542a107-a3a9-40eb-93e5-ef0f50355ef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************Autoencoder bottleneck size 2***************\n",
      "[1, test] loss: 90.189\n",
      "[1, 0] loss: 91.768\n",
      "[2, test] loss: 53.590\n",
      "[2, 0] loss: 53.133\n",
      "[3, test] loss: 53.535\n",
      "[3, 0] loss: 52.375\n",
      "[4, test] loss: 53.441\n",
      "[4, 0] loss: 53.638\n",
      "[5, test] loss: 53.488\n",
      "[5, 0] loss: 53.553\n",
      "[6, test] loss: 53.529\n",
      "[6, 0] loss: 53.808\n",
      "[7, test] loss: 53.463\n",
      "[7, 0] loss: 53.843\n",
      "[8, test] loss: 53.475\n",
      "[8, 0] loss: 52.555\n",
      "[9, test] loss: 53.460\n",
      "[9, 0] loss: 51.315\n",
      "[10, test] loss: 53.529\n",
      "[10, 0] loss: 52.256\n",
      "[10, test] loss: 53.573\n",
      "***************Autoencoder bottleneck size 5***************\n",
      "[1, test] loss: 91.055\n",
      "[1, 0] loss: 93.222\n",
      "[2, test] loss: 46.335\n",
      "[2, 0] loss: 44.590\n",
      "[3, test] loss: 42.626\n",
      "[3, 0] loss: 44.064\n",
      "[4, test] loss: 41.153\n",
      "[4, 0] loss: 39.162\n",
      "[5, test] loss: 39.752\n",
      "[5, 0] loss: 38.617\n",
      "[6, test] loss: 39.180\n",
      "[6, 0] loss: 38.470\n",
      "[7, test] loss: 40.017\n",
      "[7, 0] loss: 40.171\n",
      "[8, test] loss: 38.858\n",
      "[8, 0] loss: 40.320\n",
      "[9, test] loss: 38.882\n",
      "[9, 0] loss: 37.639\n",
      "[10, test] loss: 38.714\n",
      "[10, 0] loss: 38.666\n",
      "[10, test] loss: 37.969\n",
      "***************Autoencoder bottleneck size 10***************\n",
      "[1, test] loss: 88.450\n",
      "[1, 0] loss: 88.846\n",
      "[2, test] loss: 31.834\n",
      "[2, 0] loss: 30.109\n",
      "[3, test] loss: 27.759\n",
      "[3, 0] loss: 28.155\n",
      "[4, test] loss: 26.129\n",
      "[4, 0] loss: 27.129\n",
      "[5, test] loss: 25.488\n",
      "[5, 0] loss: 25.392\n"
     ]
    }
   ],
   "source": [
    "bottleneck_sizes = [2, 5, 10, 50]\n",
    "autoencoders = {}\n",
    "min_loss = float(\"inf\")\n",
    "for bottleneck_size in bottleneck_sizes:\n",
    "    training_loss = []    # Here we save the training process.\n",
    "    validation_loss = []  # And here the validation loss after each epoch\n",
    "\n",
    "    print(\"*\"*15 + f\"Autoencoder bottleneck size {bottleneck_size}\" + \"*\"*15)\n",
    "  \n",
    "    autoencoder = ConvolutionalAutoencoderReducedLatentDim(\n",
    "        input_shape=(28, 28),\n",
    "        n_blocks=2,\n",
    "        downsampling_method='max-pooling',\n",
    "        upsampling_method='nearest',\n",
    "        layers_per_block=2,\n",
    "        latent_dimensionality=bottleneck_size\n",
    "    ).to(device)\n",
    "    adam = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(EPOCHS_AUTOENCODER):\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = test(autoencoder, val_dataloader, mse, epoch, validation_loss)\n",
    "        if loss < min_loss:\n",
    "            torch.save({'encoder': autoencoder.encoder.state_dict(),\n",
    "                        'bottleneck_size': bottleneck_size}, 'encoder.pt')\n",
    "        autoencoder.train()\n",
    "        train(autoencoder, train_dataloader, mse, adam, epoch, training_loss, 500)\n",
    "        \n",
    "        # if early_stopping(validation_loss):\n",
    "        #     break\n",
    "\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        test(autoencoder, val_dataloader, mse, epoch, validation_loss)\n",
    "    autoencoders[f\"bottleneck_{bottleneck_size}\"] = {\"val_loss\": validation_loss, \"model\": autoencoder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "ySmvQV1_PQML",
    "outputId": "d1c46b9c-2c76-423b-a940-d0b57f50c2e5"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for bottleneck_size in autoencoders:\n",
    "    plt.plot(autoencoders[bottleneck_size][\"val_loss\"], label=bottleneck_size.replace(\"_\", \"=\"))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUdfmbcuEoh-"
   },
   "source": [
    "### Average image reconstruction loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CRodHIYQEoh_"
   },
   "source": [
    "This section presents the average image reconstruction loss for the different autoencoders from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AWw5Pz6hEoiA"
   },
   "outputs": [],
   "source": [
    "def get_num_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "def get_avg_image_reconstruction_loss(model, dataloader, criterion):\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    for batch in dataloader:\n",
    "        images = batch[0].to(device)\n",
    "        reconstructed = model(images)\n",
    "        loss += criterion(reconstructed, images).item()\n",
    "        total += images.shape[0]\n",
    "\n",
    "    mean_loss = loss / total\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "7m-KrRnwEoiC",
    "outputId": "9d793e94-bf57-4a91-b683-4d45ceaf2478"
   },
   "outputs": [],
   "source": [
    "tbl = PrettyTable()\n",
    "tbl.field_names = [\"Model\", \"Number of parameters\", \"Validation loss\", \"Test loss\"]\n",
    "for model_name, model_info in autoencoders.items():\n",
    "    autoencoder = model_info[\"model\"]\n",
    "    num_params = get_num_parameters(autoencoder)\n",
    "    val_loss = get_avg_image_reconstruction_loss(autoencoder, val_dataloader, mse)\n",
    "    test_loss = get_avg_image_reconstruction_loss(autoencoder, test_dataloader, mse)\n",
    "    tbl.add_row([model_name, num_params, val_loss, test_loss])\n",
    "\n",
    "print(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yA_kBSDuEoiF"
   },
   "source": [
    "Generating random images taking 5 elements from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cJEtvBU5EoiF",
    "outputId": "9a1be508-cdb0-4da6-8256-751c19b75450"
   },
   "outputs": [],
   "source": [
    "autoencoder = autoencoders[\"bottleneck_50\"][\"model\"]\n",
    "random_indexes = [randint(0,len(test_dataloader)) for _ in range(5)]\n",
    "for index in random_indexes:\n",
    "    plt.figure()\n",
    "    show_image(mnist_test[index][0], autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LXq7cHWEoiI"
   },
   "source": [
    "Presenting 5 random generated images to the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNrIKhXZHSb7"
   },
   "outputs": [],
   "source": [
    "def generate_image():\n",
    "    noise = torch.randn((1, 50)).to(device)\n",
    "    generated = autoencoder.convolutional_decoder(autoencoder.linear_decoder(noise).view((1,) + autoencoder.encoder_output_shape))\n",
    "    plt.imshow(generated.squeeze().detach().cpu().numpy(), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6N-oELrGEoiI",
    "outputId": "462e1394-7efe-4d30-dcde-8016e6784ec4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    plt.figure()\n",
    "    generate_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04ODN1xHIHEL"
   },
   "source": [
    "# Exercise 2: Transfer Learning\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. Select a subset of 100 images and their associated labels from the MNIST training data.\n",
    "2. Select one of the previously trained autoencoders.\n",
    "3. Create a digit (0-9) classification model reusing the encoder of the autoencoder and adding the needed fully connected (projection) layer.\n",
    "3. Pretraining: use the weights of the autoencoder as initial values for the network weights and train a classification model on the subset of 100 samples.\n",
    "4. Fine-tuning: do the same, but train the new projection layer with a normal learning rate and the reused part with a very low learning rate.\n",
    "5. From scratch: train the model on the 100 samples without reusing the decoder weights at all.\n",
    "6. Show the accuracy of the four models on the MNIST test set in a table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__skvBbtjABn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0jvg9aSh70E"
   },
   "source": [
    "We don't need to define a softmax layer, as the `CrossEntropyLoss` class internally applies it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6AdTRqxEoiP"
   },
   "outputs": [],
   "source": [
    "# Load saved autoencoder training checkpoint.\n",
    "autoencoder_checkpoint = torch.load('encoder.pt')\n",
    "bottleneck_size = autoencoder_checkpoint['bottleneck_size']\n",
    "encoder = ConvolutionalAutoencoderReducedLatentDim(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=bottleneck_size\n",
    ").encoder.to(device)\n",
    "encoder.load_state_dict(autoencoder_checkpoint['encoder'])\n",
    "\n",
    "# Define classification layer.\n",
    "classifier = torch.nn.Linear(bottleneck_size, 10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqvqjkE5h2jR"
   },
   "source": [
    "We use the cross-entropy as our loss function and Adam as our optimization algorithm with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adq2d7EnJaW5"
   },
   "outputs": [],
   "source": [
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "optimizer_adam = torch.optim.Adam(classifier.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GyA7pTDyiZH8"
   },
   "source": [
    "We create a subset dataset using the pytorch's `Subset` class with the first 100 samples from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8UWsjOhJqKN"
   },
   "outputs": [],
   "source": [
    "training_subset = torch.utils.data.Subset(mnist_train, range(100))\n",
    "subset_dataloader = torch.utils.data.DataLoader(training_subset,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrkIqayBiubF"
   },
   "source": [
    "Define our `classification_train` function that will be called at each epoch. The function takes the classifier network as well as the encoder network. It first projects the input image to the latent space and then applies the classification layer.\n",
    "\n",
    "The output of the encoder is detached so the gradients are not computed (because we dont need them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWTRqkZ6MEgf"
   },
   "outputs": [],
   "source": [
    "def classification_train(classifier, encoder, dataloader, criterion, optimizer, epoch, loss_history):\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device, torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        # To latent space.\n",
    "        codes = encoder(images).detach()\n",
    "        # Classify from latent space.\n",
    "        outputs = classifier(codes)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()   # Backprop.\n",
    "        optimizer.step()  # Parameter updates.\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        print('[%d, %d] loss: %.3f' % (epoch + 1, i, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BL_Pz6KhizE8"
   },
   "source": [
    "Very similar to the `classification_train` function, we define our `classification_test` function to validate. In this case we don't backpropagate and update our parameters. We also compute the accuracy by summing all the correct predictions and dividing by the total number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv5E5Xc6QiKV"
   },
   "outputs": [],
   "source": [
    "def classification_test(classifier, encoder, dataloader, criterion, epoch, loss_history):\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device, torch.long)\n",
    "        codes = encoder(images)\n",
    "        outputs = classifier(codes)\n",
    "        loss += criterion(outputs, labels).item()\n",
    "        # Compute average accuracy.\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    mean_loss = loss / (i + 1)\n",
    "    acc = 100 * correct / total\n",
    "    loss_history.append(mean_loss)  \n",
    "    print('[%d, test] loss: %.3f accuracy: %.3f' % (epoch + 1, mean_loss, acc))\n",
    "    return mean_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fub5vuTJkmGN"
   },
   "source": [
    "We pass the `encoder` submodule of the `autoencoder` object as the `encoder` parameter of the above-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "Tx50jNQdNaoX",
    "outputId": "499335b3-549a-4393-ff1c-2d9a0314576b"
   },
   "outputs": [],
   "source": [
    "pretraining_training_loss = []\n",
    "pretraining_validation_loss = []  # Here we save the training process.\n",
    "min_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS_PRETRAINING):\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        loss, _ = classification_test(classifier, encoder, val_dataloader, cross_entropy, epoch, pretraining_validation_loss)\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            torch.save({'classifier': classifier.state_dict(),\n",
    "                        'encoder': encoder.state_dict()},\n",
    "                       'best_pretrain.pt')\n",
    "    classifier.train()\n",
    "    classification_train(classifier, encoder, subset_dataloader, cross_entropy, optimizer_adam, epoch, pretraining_training_loss)\n",
    "\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    loss, _ = classification_test(classifier, encoder, val_dataloader, cross_entropy, epoch, pretraining_validation_loss)\n",
    "    if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            torch.save({'classifier': classifier.state_dict(),\n",
    "                        'encoder': encoder.state_dict()},\n",
    "                       'best_pretrain.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "3g7sNGZChB9G",
    "outputId": "d67495b9-233e-41dc-83d1-be793e1bba4f"
   },
   "outputs": [],
   "source": [
    "plt.title('Pretraining classification loss curves')\n",
    "plt.plot(pretraining_training_loss, label='training loss')\n",
    "plt.plot(np.linspace(0, len(pretraining_training_loss), len(pretraining_validation_loss)), pretraining_validation_loss, '-x', label='validation loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cross entropy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8HSHSVvRmjS"
   },
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(mnist_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B2dNc7aeap_8",
    "outputId": "5d4b550f-ccc5-44bf-a995-fb47f7214af0"
   },
   "outputs": [],
   "source": [
    "classification_test(classifier, encoder, test_dataloader, cross_entropy, epoch, []);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27T_v5UkjACE"
   },
   "source": [
    "## Pretrain + Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZgsgSfHmYWH"
   },
   "source": [
    "We modify the optimizer in order to have different learning rates for the encoder and the classifier.\n",
    "\n",
    "The `classification_test` function doesn't need to be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZUYo_X3m9D3"
   },
   "source": [
    "We define a new Adam optimizer for the encoder network with a small learning rate of 0.0001. The classification network optimizer is reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-PPua7XNEoiy"
   },
   "outputs": [],
   "source": [
    "optimizer_adam = torch.optim.Adam([\n",
    "    {\"params\": classifier.parameters()},\n",
    "    {\"params\": encoder.parameters(), \"lr\":0.0001}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxBX3gRtnOV9"
   },
   "source": [
    "Note that we don't redefine the `training_loss` and `validation_loss` list, so they will have the values of the pretraining stage too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "RdqDkdgwjACN",
    "outputId": "060208e4-242e-40e5-b611-e047ba9f7b27"
   },
   "outputs": [],
   "source": [
    "min_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS_FINETUNING):\n",
    "    classifier.eval()\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        loss, _ = classification_test(classifier, encoder, val_dataloader, cross_entropy, epoch, pretraining_validation_loss)\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            torch.save({'classifier': classifier.state_dict(),\n",
    "                        'encoder': encoder.state_dict()},\n",
    "                       'best_pretraining+fine-tuning.pt')\n",
    "    classifier.train()\n",
    "    encoder.train()\n",
    "    classification_train(classifier, encoder, subset_dataloader,\n",
    "                         cross_entropy, optimizer_adam, epoch, pretraining_training_loss)\n",
    "\n",
    "classifier.eval()\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    loss, _ = classification_test(classifier, encoder, val_dataloader, cross_entropy, epoch, pretraining_validation_loss)\n",
    "    if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            torch.save({'classifier': classifier.state_dict(),\n",
    "                        'encoder': encoder.state_dict()},\n",
    "                       'best_pretraining+fine-tuning.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ge4JD9dpaYXZ",
    "outputId": "52817669-9ad4-4b6b-fb51-ab3af70ae609"
   },
   "outputs": [],
   "source": [
    "plt.title('Fine-tunning classification loss curves')\n",
    "plt.plot(pretraining_training_loss, label='training loss')\n",
    "plt.plot(np.linspace(0, len(pretraining_training_loss), len(pretraining_validation_loss)), pretraining_validation_loss, '-x', label='validation loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cross entropy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zMB42WyHPNmA",
    "outputId": "02c1a40d-0633-45bf-a4ef-83aff389c702"
   },
   "outputs": [],
   "source": [
    "classification_test(classifier, encoder, test_dataloader, cross_entropy, epoch, []);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6bOuf5XxEoi6"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thwBf-MrEoi7"
   },
   "outputs": [],
   "source": [
    "encoder = ConvolutionalAutoencoderReducedLatentDim(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=bottleneck_size\n",
    ").encoder.to(device)\n",
    "encoder.load_state_dict(autoencoder_checkpoint[\"encoder\"])\n",
    "# Define classification layer.\n",
    "classifier = torch.nn.Linear(bottleneck_size, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7dpDhjUMbx0"
   },
   "outputs": [],
   "source": [
    "optimizer_adam = torch.optim.Adam([\n",
    "    {'params': classifier.parameters(), 'lr': 1e-2},\n",
    "    {'params': encoder.parameters(), 'lr': 1e-5}\n",
    "], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "77x5A8L9Eoi8",
    "outputId": "7fdbbfeb-3e3f-45da-9e5e-0244c2245cf0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finetuning_training_loss = []\n",
    "finetuning_validation_loss = []\n",
    "min_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS_PRETRAINING + EPOCHS_FINETUNING):\n",
    "    classifier.eval()\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        loss, _ = classification_test(classifier, encoder, val_dataloader, cross_entropy, epoch, finetuning_validation_loss)\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            torch.save({'classifier': classifier.state_dict(),\n",
    "                        'encoder': encoder.state_dict()},\n",
    "                       'best_fine-tuning.pt')\n",
    "    classifier.train()\n",
    "    encoder.train()\n",
    "    classification_train(classifier, encoder, subset_dataloader,\n",
    "                         cross_entropy, optimizer_adam, epoch, finetuning_training_loss)\n",
    "\n",
    "classifier.eval()\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    loss, _ = classification_test(classifier, encoder, val_dataloader, cross_entropy, epoch, finetuning_validation_loss)\n",
    "    if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            torch.save({'classifier': classifier.state_dict(),\n",
    "                        'encoder': encoder.state_dict()},\n",
    "                       'best_fine-tuning.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ALjFferxPcR4",
    "outputId": "bfaa18f9-b1dc-4697-e784-25cf3d5dd59e"
   },
   "outputs": [],
   "source": [
    "plt.title('Fine-tunning classification loss curves')\n",
    "plt.plot(finetuning_training_loss, label='training loss')\n",
    "plt.plot(np.linspace(0, len(finetuning_training_loss), len(finetuning_validation_loss)), finetuning_validation_loss, '-x', label='validation loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cross entropy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3mVz45k1PWee"
   },
   "source": [
    "## From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8s58B_XoSis"
   },
   "outputs": [],
   "source": [
    "new_autoencoder = ConvolutionalAutoencoderReducedLatentDim(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=50\n",
    ").to(device)\n",
    "\n",
    "new_classifier = torch.nn.Sequential(\n",
    "    new_autoencoder.encoder,\n",
    "    torch.nn.Linear(50, 10)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DjdCLKOPqBJ"
   },
   "outputs": [],
   "source": [
    "def new_classification_train(classifier, dataloader, criterion, classifier_optimizer, epoch, loss_history):\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device, torch.long)\n",
    "        classifier_optimizer.zero_grad()\n",
    "        # To latent space + Classify from latent space.\n",
    "        outputs = classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()              # Backprop.\n",
    "        classifier_optimizer.step()  # Update parameters.\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        print('[%d, %d] loss: %.3f' % (epoch + 1, i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJnplDxAQCJQ"
   },
   "outputs": [],
   "source": [
    "def new_classification_test(classifier, dataloader, criterion, epoch, loss_history):\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device, torch.long)\n",
    "        outputs = classifier(images)\n",
    "        loss += criterion(outputs, labels).item()\n",
    "        # Compute average accuracy.\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    mean_loss = loss / (i + 1)\n",
    "    acc = 100 * correct / total\n",
    "    loss_history.append(mean_loss)  \n",
    "    print('[%d, validation] loss: %.3f accuracy: %.3f' % (epoch + 1, mean_loss, acc))\n",
    "    return mean_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iHljTyDkQINO"
   },
   "outputs": [],
   "source": [
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "new_classifier_adam = torch.optim.Adam(new_classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Po-AZQH0QSsc",
    "outputId": "32a9f711-1dc0-476d-da13-53d5b1a98407"
   },
   "outputs": [],
   "source": [
    "new_training_loss = []\n",
    "new_validation_loss = []\n",
    "min_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS_AUTOENCODER + EPOCHS_PRETRAINING + EPOCHS_FINETUNING):\n",
    "    new_classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        loss, _ = new_classification_test(new_classifier, val_dataloader, cross_entropy, epoch, new_validation_loss)\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            torch.save(new_classifier.state_dict(), 'best_from-scratch.pt')\n",
    "    new_classifier.train()\n",
    "    new_classification_train(new_classifier, subset_dataloader,\n",
    "                         cross_entropy, new_classifier_adam, epoch, new_training_loss)\n",
    "\n",
    "new_classifier.eval()\n",
    "with torch.no_grad():\n",
    "    loss, _ = new_classification_test(new_classifier, val_dataloader, cross_entropy, epoch, new_validation_loss)\n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        torch.save(new_classifier.state_dict(), 'best_from-scratch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "q1sJt-axRznj",
    "outputId": "6361785f-8d52-4fd5-fbb9-152d69705263",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('From scratch classification loss curves')\n",
    "plt.plot(new_training_loss, label='training loss')\n",
    "plt.plot(np.linspace(0, len(new_training_loss), len(new_validation_loss)), new_validation_loss, '-x', label='validation loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cross entropy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWd8_HtUSguK"
   },
   "source": [
    "This learning curves show clear symptoms of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f9GyHMWoRDjZ",
    "outputId": "aace7fdd-8cce-4684-f866-998cebff1e2a"
   },
   "outputs": [],
   "source": [
    "new_classification_test(new_classifier, test_dataloader, cross_entropy, epoch, []);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAs0TTL4VXcM"
   },
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSHVbYMIcWKi"
   },
   "source": [
    "Load best models (early-stopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-Mvz-UgVZ0j",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder_pretrain = ConvolutionalAutoencoderReducedLatentDim(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=bottleneck_size\n",
    ").to(device)\n",
    "classifier_pretrain = torch.nn.Linear(bottleneck_size, 10).to(device)\n",
    "\n",
    "pretrain_checkpoint = torch.load('best_pretrain.pt')\n",
    "autoencoder_pretrain.encoder.load_state_dict(pretrain_checkpoint['encoder'])\n",
    "classifier_pretrain.load_state_dict(pretrain_checkpoint['classifier'])\n",
    "\n",
    "autoencoder_finetuning = ConvolutionalAutoencoderReducedLatentDim(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=bottleneck_size\n",
    ").to(device)\n",
    "classifier_finetuning = torch.nn.Linear(bottleneck_size, 10).to(device)\n",
    "\n",
    "finetuning_checkpoint = torch.load('best_fine-tuning.pt')\n",
    "autoencoder_finetuning.encoder.load_state_dict(finetuning_checkpoint['encoder'])\n",
    "classifier_finetuning.load_state_dict(finetuning_checkpoint['classifier'])\n",
    "\n",
    "autoencoder_pretrain_finetuning = ConvolutionalAutoencoderReducedLatentDim(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=bottleneck_size\n",
    ").to(device)\n",
    "classifier_pretrain_finetuning= torch.nn.Linear(bottleneck_size, 10).to(device)\n",
    "\n",
    "pretrain_finetuning_checkpoint = torch.load('best_pretraining+fine-tuning.pt')\n",
    "autoencoder_pretrain_finetuning.encoder.load_state_dict(pretrain_finetuning_checkpoint['encoder'])\n",
    "classifier_pretrain_finetuning.load_state_dict(pretrain_finetuning_checkpoint['classifier'])\n",
    "\n",
    "autoencoder_fromscratch = ConvolutionalAutoencoderReducedLatentDim(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=bottleneck_size\n",
    ").to(device)\n",
    "\n",
    "classifier_fromscratch = torch.nn.Sequential(\n",
    "    autoencoder_fromscratch.encoder,\n",
    "    torch.nn.Linear(bottleneck_size, 10)\n",
    ").to(device)\n",
    "\n",
    "classifier_fromscratch.load_state_dict(torch.load('best_from-scratch.pt'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_llxE7lmcqDZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare cross entropy loss and accuracy on validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "SIz4O90lcsdK",
    "outputId": "e02adb6e-2144-48b9-d159-b50234a138e2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tbl = PrettyTable()\n",
    "tbl.field_names = [\"model\",\"validation loss\", \"validation acc.\", \"test loss\", \"test acc.\"]\n",
    "pair_models = {\"pretrained\": (classifier_pretrain, autoencoder_pretrain.encoder), \n",
    "               \"pretrain+finetuning\": (classifier_pretrain_finetuning, autoencoder_pretrain_finetuning.encoder),\n",
    "               \"finetuned\": (classifier_finetuning, autoencoder_finetuning.encoder)\n",
    "              }\n",
    "losses = {\"pretrained\": None, \"pretrain+finetuning\": None, \"finetuned\": None, \"scratch\": None}\n",
    "for name, models in pair_models.items():\n",
    "    print(name + \" train:\")\n",
    "    val_loss, val_acc = classification_test(models[0], models[1], val_dataloader, cross_entropy, epoch, [])\n",
    "    print(name + \" test:\")\n",
    "    test_loss, test_acc = classification_test(models[0], models[1], test_dataloader, cross_entropy, epoch, [])\n",
    "    losses[name] = test_loss\n",
    "    tbl.add_row([name,val_loss,val_acc,test_loss,test_acc])\n",
    "\n",
    "print(\"scratch train\")\n",
    "val_loss, val_acc = new_classification_test(classifier_fromscratch, val_dataloader, cross_entropy, epoch, [])\n",
    "print(\"scratch test\")\n",
    "test_loss, test_acc = new_classification_test(classifier_fromscratch, test_dataloader, cross_entropy, epoch, [])\n",
    "losses[\"scratch\"] = test_loss\n",
    "tbl.add_row(\n",
    "    [\"scratch\", val_loss, val_acc, test_loss, test_acc])\n",
    "print(tbl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "bT6u4qk0EojV",
    "outputId": "9c8f3c12-bd22-42af-a9a9-a5137073303f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_names = tuple(losses.keys())\n",
    "loss = tuple(losses.values())\n",
    "y_pos = np.arange(len(loss))\n",
    "\n",
    "plt.bar(y_pos, loss, color=(\"r\", \"g\", \"b\", \"y\"))\n",
    "plt.xticks(y_pos, models_names)\n",
    "plt.xlabel(\"Model names\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Models loss, the lower the better\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "v-KZMZkkP6tw",
    "outputId": "b647f505-c10d-40ee-bcb2-a5617b6756d3"
   },
   "outputs": [],
   "source": [
    "plt.title('classification loss curves')\n",
    "plt.plot(pretraining_validation_loss, '-x', label='pretraining loss')\n",
    "plt.plot(finetuning_validation_loss, '-o', label='finetuning loss')\n",
    "plt.plot(new_validation_loss, '-+', label='from-scratch loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cross entropy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIvHHXWBqZe2"
   },
   "source": [
    "We expected to see a better performance on the model trained with the pretrain+finetuning proceidure, but the model trained from scratch actually performs slightly better on the validation and test set. If the model had more parameters we would expect the model trained from scratch to not generalize on the validation nor test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ns7oGWd2EZVY"
   },
   "source": [
    "## Optional Exercise A: Variational Autoencoder\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Implement an autoencoder like that from Exercise 1, but turning the deterministic bottleneck into a stochastic bottleneck, with an isotropic Gaussian as distribution for the latent variables.\n",
    "2. Train the model optimizing the Evidence Lower Bound (ELBO).\n",
    "3. Generate samples with the decoder and show them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFiIlaOnOCzC"
   },
   "source": [
    "New class for variational autoencoder based on the previous autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2LJFzsBeSC6k"
   },
   "outputs": [],
   "source": [
    "class ConvolutionalVAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, n_blocks, downsampling_method, upsampling_method,\n",
    "                 layers_per_block=2, latent_dimensionality=50):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.downsampling_method = downsampling_method\n",
    "        self.upsampling_method = upsampling_method\n",
    "        self.latent_dimensionality = latent_dimensionality\n",
    "\n",
    "        # Encoder: Convolutional blocks + Linear\n",
    "        self.convolutional_encoder = ConvolutionalEncoder(\n",
    "            n_blocks, downsampling_method, layers_per_block=layers_per_block)\n",
    "        self.encoder_output_shape = (self.convolutional_encoder.init_filters * 2 ** (n_blocks - 1),\n",
    "                                     input_shape[0] // 2 ** n_blocks,\n",
    "                                     input_shape[1] // 2 ** n_blocks)\n",
    "        self.mu = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(self.encoder_output_shape[0] * self.encoder_output_shape[1] * self.encoder_output_shape[2],\n",
    "                            latent_dimensionality),\n",
    "        )\n",
    "        self.log_var = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(self.encoder_output_shape[0] * self.encoder_output_shape[1] * self.encoder_output_shape[2],\n",
    "                            latent_dimensionality),\n",
    "        )\n",
    "\n",
    "        # Decoder: Linear + Convolutional blocks\n",
    "        self.linear_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dimensionality,\n",
    "                            self.encoder_output_shape[0] * self.encoder_output_shape[1] * self.encoder_output_shape[2]),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.convolutional_decoder = ConvolutionalDecoder(\n",
    "            n_blocks, upsampling_method,\n",
    "            self.convolutional_encoder.output_channels, layers_per_block,\n",
    "            activation=\"sigmoid\")\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        return torch.randn_like(std).mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_output = self.convolutional_encoder(x)\n",
    "        # Get mean and variance.\n",
    "        mu = self.mu(encoder_output)\n",
    "        log_var = self.log_var(encoder_output)\n",
    "        # Sample from a normal distribution of mean mu and variance var.\n",
    "        code = self.reparameterize(mu, log_var)\n",
    "\n",
    "        reconstruction = self.convolutional_decoder(\n",
    "            self.linear_decoder(code).view((-1,) + self.encoder_output_shape))\n",
    "        \n",
    "        # Return reconstruction and mean and variance for the loss computation.  \n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_86btVGNP-e4"
   },
   "source": [
    "Reconstruction + KL divergence losses summed over all elements and batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubi766EfQJu0"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJa6GgbZOB4a"
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = binary_cross_entropy(recon_x.view(-1, 28 * 28), x.view(-1, 28 * 28), reduction='sum')\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(vae, dataloader, optimizer, epoch, loss_history, log_interval=100):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        images = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, mu, log_var = vae(images)\n",
    "        # Divide by batch size to compute mean image reconstruction error.\n",
    "        loss = loss_function(reconstructed, images, mu, log_var)\n",
    "        loss.backward()   # Backprop.\n",
    "        optimizer.step()  # Parameter updates.\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        if i % log_interval == 0:\n",
    "            print('[%d, %d] loss: %.3f' % (epoch + 1, i, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vae = ConvolutionalVAE(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=50\n",
    ").to(device)\n",
    "\n",
    "adam = torch.optim.Adam(vae.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_loss = []    #\n",
    "validation_loss = []  # Here we save the training process.\n",
    "\n",
    "vae.train()\n",
    "for epoch in range(EPOCHS_VAE):\n",
    "    train(vae, train_dataloader, adam, epoch, training_loss)\n",
    "\n",
    "vae.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_image(vae):\n",
    "    noise = torch.randn((1, 50)).to(device)\n",
    "    generated = vae.convolutional_decoder(vae.linear_decoder(noise).view((1,) + vae.encoder_output_shape))\n",
    "    plt.imshow(generated.squeeze().detach().cpu().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    plt.figure()\n",
    "    generate_image(vae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bm5ubKrARmUW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhlyRO23Eojd"
   },
   "outputs": [],
   "source": [
    "vae = ConvolutionalVAE(\n",
    "    input_shape=(28, 28),\n",
    "    n_blocks=2,\n",
    "    downsampling_method='max-pooling',\n",
    "    upsampling_method='nearest',\n",
    "    layers_per_block=2,\n",
    "    latent_dimensionality=50\n",
    ").to(device)\n",
    "\n",
    "adam = torch.optim.Adam(vae.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rAOe-6dVRF8d",
    "outputId": "7728af62-bb9c-44e3-c9da-30bbe0f628d8"
   },
   "outputs": [],
   "source": [
    "training_loss = []    #\n",
    "validation_loss = []  # Here we save the training process.\n",
    "\n",
    "vae.train()\n",
    "for epoch in range(EPOCHS_VAE):\n",
    "    train(vae, train_dataloader, adam, epoch, training_loss)\n",
    "\n",
    "vae.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qVgx9VbSSOo"
   },
   "outputs": [],
   "source": [
    "def generate_image(vae):\n",
    "    noise = torch.randn((1, 50)).to(device)\n",
    "    generated = vae.convolutional_decoder(vae.linear_decoder(noise).view((1,) + vae.encoder_output_shape))\n",
    "    plt.imshow(generated.squeeze().detach().cpu().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dgM2Upt6Liwt",
    "outputId": "ce0919b3-2569-4f56-c5ec-ce22c984a63c"
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    plt.figure()\n",
    "    generate_image(vae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1h4e45LLon2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "team08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
